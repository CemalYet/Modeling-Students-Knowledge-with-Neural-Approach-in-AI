{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import deepkt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as ac; ac.set_theme()\n",
    "import scipy.stats as st\n",
    "from sci_analysis import analyze\n",
    "import pandas as pd\n",
    "import bnlearn as bn\n",
    "from scipy.special import expit\n",
    "import matplotlib\n",
    "import pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id     log_id  skill_id       old  correct\n",
      "0         50121  167478035      7014  0.000000        0\n",
      "1         50121  167478043      7014  1.000000        1\n",
      "2         50121  167478053      7014  1.000000        1\n",
      "3         50121  167478069      7014  1.000000        1\n",
      "4         50964  167478041      7014  1.000000        1\n",
      "...         ...        ...       ...       ...      ...\n",
      "708626   362374  175292695     31260  0.666667        1\n",
      "708627   362374  175293309     31260  0.666667        1\n",
      "708628   362374  175293826     31260  1.000000        1\n",
      "708629   362374  175294002     31260  1.000000        1\n",
      "708630   362374  175294060     31260  1.000000        1\n",
      "\n",
      "[708631 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "an = \"data/2015_100_skill_builders_main_problems.csv\"\n",
    "df = pd.read_csv ('data/2015_100_skill_builders_main_problems.csv')\n",
    "df[\"correct\"]= np.where(df[\"old\"]>0.5, 1,0)\n",
    "df.to_csv('data/2015_100_skill_builders_main_problems.csv')\n",
    "\n",
    "print(df)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of DKT:\n",
    "#### Part 1: Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dataset path\n",
    "fn = \"data/2015_100_skill_builders_main_problems.csv\"\n",
    "verbose = 1 # Verbose = {0,1,2}\n",
    "best_model_weights = \"weights/bestmodel\" # File to save the model.\n",
    "log_dir = \"logs\" # Path to save the logs.\n",
    "optimizer = \"adam\" # Optimizer to use\n",
    "lstm_units = 110 # Number of LSTM units\n",
    "batch_size = 32 # Batch size\n",
    "epochs = 20 # Number of epochs to train\n",
    "dropout_rate = 0.4 # Dropout rate\n",
    "test_fraction = 0.2 # Portion of data to be used for testing\n",
    "validation_fraction = 0.2 # Portion of training data to be used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Data Summary =============\n",
      "Total number of students: 19424\n",
      "Training set size: 12431\n",
      "Validation set size: 3107\n",
      "Testing set size: 3884\n",
      "Number of skills: 100\n",
      "Number of features in the input: 200\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from deepkt import deepkt, data_util, metrics\n",
    "\n",
    "\n",
    "dataset, length, nb_features, nb_skills = data_util.load_dataset(fn=fn,\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 shuffle=True)\n",
    "\n",
    "train_set, test_set, val_set = data_util.split_dataset(dataset=dataset,\n",
    "                                                       total_size=length,\n",
    "                                                       test_fraction=test_fraction,\n",
    "                                                       val_fraction=validation_fraction)\n",
    "\n",
    "\n",
    "set_sz = length * batch_size\n",
    "test_set_sz = (set_sz * test_fraction)\n",
    "val_set_sz = (set_sz - test_set_sz) * validation_fraction\n",
    "train_set_sz = set_sz - test_set_sz - val_set_sz\n",
    "print(\"============= Data Summary =============\")\n",
    "print(\"Total number of students: %d\" % set_sz)\n",
    "print(\"Training set size: %d\" % train_set_sz)\n",
    "print(\"Validation set size: %d\" % val_set_sz)\n",
    "print(\"Testing set size: %d\" % test_set_sz)\n",
    "print(\"Number of skills: %d\" % nb_skills)\n",
    "print(\"Number of features in the input: %d\" % nb_features)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DKTModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 200)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 110)         136840    \n",
      "_________________________________________________________________\n",
      "outputs (TimeDistributed)    (None, None, 100)         11100     \n",
      "=================================================================\n",
      "Total params: 147,940\n",
      "Trainable params: 147,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#batch_size=1\n",
    "\n",
    "train_length=train_set_sz // batch_size\n",
    "val_length=val_set_sz // batch_size\n",
    "\n",
    "student_model = deepkt.DKTModel(\n",
    "                        nb_features=nb_features,\n",
    "                        nb_skills=nb_skills,\n",
    "                        hidden_units=lstm_units,\n",
    "                        dropout_rate=dropout_rate)\n",
    "\n",
    "student_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(),\n",
    "            metrics.AUC(),\n",
    "            metrics.Precision(),\n",
    "            metrics.Recall()\n",
    "        ])\n",
    "\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 12:45:47.296956: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_18092_18689' and '__inference___backward_standard_lstm_18092_18689_specialized_for_training_1_Adam_gradients_gradients_lstm_1_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_19341' both implement 'lstm_11f472e9-584c-4b69-b11d-adc143e7bd13' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 4s 4s/step - loss: 0.1102 - binary_accuracy: 0.5199 - auc_1: 0.4626 - precision_1: 0.6947 - recall_1: 0.5869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 12:45:48.048685: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.791813). Check your callbacks.\n",
      "388/388 [==============================] - 91s 235ms/step - loss: 0.1257 - binary_accuracy: 0.7283 - auc_1: 0.6190 - precision_1: 0.7372 - recall_1: 0.9768 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.0000e+00 - val_auc_1: 0.0000e+00 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "388/388 [==============================] - 85s 220ms/step - loss: 0.1227 - binary_accuracy: 0.7399 - auc_1: 0.6774 - precision_1: 0.7488 - recall_1: 0.9704 - val_loss: 0.1200 - val_binary_accuracy: 0.7440 - val_auc_1: 0.6972 - val_precision_1: 0.7569 - val_recall_1: 0.9593\n",
      "Epoch 3/20\n",
      "388/388 [==============================] - 88s 228ms/step - loss: 0.1188 - binary_accuracy: 0.7428 - auc_1: 0.6932 - precision_1: 0.7539 - recall_1: 0.9635 - val_loss: 0.1133 - val_binary_accuracy: 0.7465 - val_auc_1: 0.7104 - val_precision_1: 0.7510 - val_recall_1: 0.9783\n",
      "Epoch 4/20\n",
      "388/388 [==============================] - 78s 200ms/step - loss: 0.1185 - binary_accuracy: 0.7433 - auc_1: 0.6989 - precision_1: 0.7548 - recall_1: 0.9614 - val_loss: 0.1159 - val_binary_accuracy: 0.7479 - val_auc_1: 0.7194 - val_precision_1: 0.7636 - val_recall_1: 0.9496\n",
      "Epoch 5/20\n",
      "388/388 [==============================] - 81s 208ms/step - loss: 0.1167 - binary_accuracy: 0.7451 - auc_1: 0.7022 - precision_1: 0.7569 - recall_1: 0.9610 - val_loss: 0.1148 - val_binary_accuracy: 0.7431 - val_auc_1: 0.7206 - val_precision_1: 0.7479 - val_recall_1: 0.9769\n",
      "Epoch 6/20\n",
      "388/388 [==============================] - 83s 215ms/step - loss: 0.1168 - binary_accuracy: 0.7439 - auc_1: 0.7067 - precision_1: 0.7561 - recall_1: 0.9584 - val_loss: 0.1129 - val_binary_accuracy: 0.7443 - val_auc_1: 0.7265 - val_precision_1: 0.7607 - val_recall_1: 0.9419\n",
      "Epoch 7/20\n",
      "388/388 [==============================] - 87s 223ms/step - loss: 0.1176 - binary_accuracy: 0.7452 - auc_1: 0.7061 - precision_1: 0.7578 - recall_1: 0.9581 - val_loss: 0.1097 - val_binary_accuracy: 0.7479 - val_auc_1: 0.7239 - val_precision_1: 0.7575 - val_recall_1: 0.9631\n",
      "Epoch 8/20\n",
      "388/388 [==============================] - 91s 236ms/step - loss: 0.1179 - binary_accuracy: 0.7458 - auc_1: 0.7096 - precision_1: 0.7586 - recall_1: 0.9570 - val_loss: 0.1124 - val_binary_accuracy: 0.7479 - val_auc_1: 0.7246 - val_precision_1: 0.7565 - val_recall_1: 0.9653\n",
      "Epoch 9/20\n",
      "388/388 [==============================] - 86s 222ms/step - loss: 0.1176 - binary_accuracy: 0.7473 - auc_1: 0.7103 - precision_1: 0.7604 - recall_1: 0.9567 - val_loss: 0.1061 - val_binary_accuracy: 0.7504 - val_auc_1: 0.7274 - val_precision_1: 0.7597 - val_recall_1: 0.9632\n",
      "Epoch 10/20\n",
      "388/388 [==============================] - 86s 221ms/step - loss: 0.1150 - binary_accuracy: 0.7485 - auc_1: 0.7124 - precision_1: 0.7615 - recall_1: 0.9572 - val_loss: 0.1128 - val_binary_accuracy: 0.7524 - val_auc_1: 0.7270 - val_precision_1: 0.7606 - val_recall_1: 0.9666\n",
      "Epoch 11/20\n",
      "388/388 [==============================] - 81s 208ms/step - loss: 0.1169 - binary_accuracy: 0.7478 - auc_1: 0.7132 - precision_1: 0.7610 - recall_1: 0.9559 - val_loss: 0.1143 - val_binary_accuracy: 0.7473 - val_auc_1: 0.7272 - val_precision_1: 0.7546 - val_recall_1: 0.9683\n",
      "Epoch 12/20\n",
      "388/388 [==============================] - 86s 221ms/step - loss: 0.1156 - binary_accuracy: 0.7474 - auc_1: 0.7128 - precision_1: 0.7607 - recall_1: 0.9554 - val_loss: 0.1156 - val_binary_accuracy: 0.7493 - val_auc_1: 0.7291 - val_precision_1: 0.7548 - val_recall_1: 0.9742\n",
      "Epoch 13/20\n",
      "388/388 [==============================] - 82s 212ms/step - loss: 0.1142 - binary_accuracy: 0.7481 - auc_1: 0.7152 - precision_1: 0.7618 - recall_1: 0.9545 - val_loss: 0.1132 - val_binary_accuracy: 0.7505 - val_auc_1: 0.7335 - val_precision_1: 0.7591 - val_recall_1: 0.9651\n",
      "Epoch 14/20\n",
      "388/388 [==============================] - 75s 192ms/step - loss: 0.1168 - binary_accuracy: 0.7483 - auc_1: 0.7159 - precision_1: 0.7618 - recall_1: 0.9546 - val_loss: 0.1141 - val_binary_accuracy: 0.7535 - val_auc_1: 0.7356 - val_precision_1: 0.7627 - val_recall_1: 0.9614\n",
      "Epoch 15/20\n",
      "388/388 [==============================] - 71s 184ms/step - loss: 0.1149 - binary_accuracy: 0.7481 - auc_1: 0.7158 - precision_1: 0.7621 - recall_1: 0.9542 - val_loss: 0.1193 - val_binary_accuracy: 0.7497 - val_auc_1: 0.7300 - val_precision_1: 0.7608 - val_recall_1: 0.9588\n",
      "Epoch 16/20\n",
      "388/388 [==============================] - 81s 210ms/step - loss: 0.1145 - binary_accuracy: 0.7486 - auc_1: 0.7183 - precision_1: 0.7627 - recall_1: 0.9531 - val_loss: 0.1080 - val_binary_accuracy: 0.7516 - val_auc_1: 0.7323 - val_precision_1: 0.7573 - val_recall_1: 0.9715\n",
      "Epoch 17/20\n",
      "388/388 [==============================] - 78s 201ms/step - loss: 0.1131 - binary_accuracy: 0.7499 - auc_1: 0.7175 - precision_1: 0.7637 - recall_1: 0.9548 - val_loss: 0.1082 - val_binary_accuracy: 0.7547 - val_auc_1: 0.7326 - val_precision_1: 0.7684 - val_recall_1: 0.9524\n",
      "Epoch 18/20\n",
      "388/388 [==============================] - 78s 202ms/step - loss: 0.1155 - binary_accuracy: 0.7500 - auc_1: 0.7200 - precision_1: 0.7645 - recall_1: 0.9519 - val_loss: 0.1078 - val_binary_accuracy: 0.7517 - val_auc_1: 0.7394 - val_precision_1: 0.7592 - val_recall_1: 0.9631\n",
      "Epoch 19/20\n",
      "388/388 [==============================] - 84s 216ms/step - loss: 0.1158 - binary_accuracy: 0.7503 - auc_1: 0.7200 - precision_1: 0.7643 - recall_1: 0.9534 - val_loss: 0.1071 - val_binary_accuracy: 0.7515 - val_auc_1: 0.7370 - val_precision_1: 0.7639 - val_recall_1: 0.9541\n",
      "Epoch 20/20\n",
      "388/388 [==============================] - 74s 192ms/step - loss: 0.1128 - binary_accuracy: 0.7511 - auc_1: 0.7198 - precision_1: 0.7651 - recall_1: 0.9535 - val_loss: 0.1090 - val_binary_accuracy: 0.7544 - val_auc_1: 0.7406 - val_precision_1: 0.7651 - val_recall_1: 0.9586\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "history = student_model.fit(dataset=train_set,\n",
    "                            epochs=epochs,\n",
    "                            verbose=verbose,\n",
    "                            validation_data=val_set,\n",
    "                            callbacks=[ \n",
    "                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n",
    "                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n",
    "                                                                   save_best_only=True,\n",
    "                                                                   save_weights_only=True),\n",
    "                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5: Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x14feedf90>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.load_weights(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    122/Unknown - 12s 99ms/step - loss: 0.1132 - binary_accuracy: 0.7503 - auc_1: 0.7264 - precision_1: 0.7594 - recall_1: 0.9641"
     ]
    }
   ],
   "source": [
    "result = student_model.evaluate(test_set, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9q/qy2dbtzx1113fyq6t6nbww300000gn/T/ipykernel_22125/1839107259.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0mstudent_desired\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mget_element_by_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_set\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m700\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudent_desired\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def get_element_by_index(data_set,candidate_index):\n",
    "    \"\"\"\n",
    "    This function gets candidate\n",
    "    by index\n",
    "    \"\"\"\n",
    "    for raw_index,candidate in enumerate(data_set):\n",
    "        if raw_index == candidate_index:\n",
    "            candidate_desired = candidate\n",
    "            return candidate_desired\n",
    "\n",
    "student_desired=get_element_by_index(test_set,700)\n",
    "print(student_desired[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 8: Experiment on tf.expand_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_desired' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9q/qy2dbtzx1113fyq6t6nbww300000gn/T/ipykernel_36747/938553829.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mstudent_prediction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudent_desired\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'student_desired' is not defined"
     ]
    }
   ],
   "source": [
    "student_prediction=tf.expand_dims(student_desired[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "sample1=np.zeros((1,100,200))#change here for 2009"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sample1[0][i][2*i+1]=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from tensorflow_core import float32\n",
    "\n",
    "\n",
    "predictedTensor=tf.convert_to_tensor(sample1,dtype=float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 9: Predict only one student from Desired Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(123, 123)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_of_one_student.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "student_prediction=tf.expand_dims(predictedTensor, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(1, 100, 200)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 114 is out of bounds for axis 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9q/qy2dbtzx1113fyq6t6nbww300000gn/T/ipykernel_36747/3073232858.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction_of_one_student\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#This is how the prediction output looks like\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudent_prediction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m#this is how the input looks like see paper intuition doubling the skill_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mprediction_of_one_student\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m114\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m#prediction of last time step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m: index 114 is out of bounds for axis 0 with size 100"
     ]
    }
   ],
   "source": [
    "prediction_of_one_student=student_model.predict(tf.squeeze(student_prediction,axis=0))[0]\n",
    "print(prediction_of_one_student.shape) #This is how the prediction output looks like\n",
    "print(tf.squeeze(student_prediction,axis=0).shape)#this is how the input looks like see paper intuition doubling the skill_size\n",
    "prediction_of_one_student[114] #prediction of last time step"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discovering Skill Relation for Benchmark Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "count=100\n",
    "averages=[]\n",
    "\n",
    "for i in range (count):\n",
    "    suma = 0\n",
    "    for j in range(count):\n",
    "        suma += prediction_of_one_student[i][j]\n",
    "    averages.append(suma/count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "relationships = []\n",
    "for i in range (count):\n",
    "    relationship = []\n",
    "    for j in range(count):\n",
    "        relationship.append(prediction_of_one_student[i][j]/averages[i])\n",
    "    relationships.append(relationship)\n",
    "\n",
    "print(len(relationships))\n",
    "print(len(relationships[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(relationships, columns=[i for i in range(100)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "         0         1         2         3         4         5         6   \\\n0  1.068345  1.040982  0.832960  1.106039  0.975467  1.214644  0.898861   \n1  0.948008  1.131836  0.798682  1.133444  0.981179  1.245664  0.901863   \n2  0.962601  1.087174  0.968669  1.115423  1.002694  1.217123  0.943973   \n3  0.942234  1.060991  0.901738  1.154064  0.997016  1.220186  0.963888   \n4  0.921719  1.047234  0.912163  1.169759  1.043633  1.209286  0.958797   \n\n         7         8         9   ...        90        91        92        93  \\\n0  0.818454  0.949138  0.955375  ...  0.960094  1.050587  0.929380  0.882661   \n1  0.739570  0.904673  0.933531  ...  0.985554  1.100311  0.962271  0.883917   \n2  0.761178  0.889098  0.932294  ...  0.948742  1.133958  0.985956  0.925330   \n3  0.773595  0.889257  0.944109  ...  0.981864  1.131911  0.968736  0.913845   \n4  0.789999  0.870221  0.980002  ...  0.988514  1.109596  0.929606  0.898687   \n\n         94        95        96        97        98        99  \n0  1.029063  0.896548  1.085141  1.036949  0.673347  0.875495  \n1  1.037016  0.858318  1.103374  1.056391  0.563158  0.781053  \n2  1.039242  0.818137  1.093405  1.043078  0.536570  0.776659  \n3  1.054825  0.845073  1.068571  1.021406  0.520027  0.762446  \n4  1.030213  0.820845  1.071640  1.009794  0.550181  0.746763  \n\n[5 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.068345</td>\n      <td>1.040982</td>\n      <td>0.832960</td>\n      <td>1.106039</td>\n      <td>0.975467</td>\n      <td>1.214644</td>\n      <td>0.898861</td>\n      <td>0.818454</td>\n      <td>0.949138</td>\n      <td>0.955375</td>\n      <td>...</td>\n      <td>0.960094</td>\n      <td>1.050587</td>\n      <td>0.929380</td>\n      <td>0.882661</td>\n      <td>1.029063</td>\n      <td>0.896548</td>\n      <td>1.085141</td>\n      <td>1.036949</td>\n      <td>0.673347</td>\n      <td>0.875495</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.948008</td>\n      <td>1.131836</td>\n      <td>0.798682</td>\n      <td>1.133444</td>\n      <td>0.981179</td>\n      <td>1.245664</td>\n      <td>0.901863</td>\n      <td>0.739570</td>\n      <td>0.904673</td>\n      <td>0.933531</td>\n      <td>...</td>\n      <td>0.985554</td>\n      <td>1.100311</td>\n      <td>0.962271</td>\n      <td>0.883917</td>\n      <td>1.037016</td>\n      <td>0.858318</td>\n      <td>1.103374</td>\n      <td>1.056391</td>\n      <td>0.563158</td>\n      <td>0.781053</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.962601</td>\n      <td>1.087174</td>\n      <td>0.968669</td>\n      <td>1.115423</td>\n      <td>1.002694</td>\n      <td>1.217123</td>\n      <td>0.943973</td>\n      <td>0.761178</td>\n      <td>0.889098</td>\n      <td>0.932294</td>\n      <td>...</td>\n      <td>0.948742</td>\n      <td>1.133958</td>\n      <td>0.985956</td>\n      <td>0.925330</td>\n      <td>1.039242</td>\n      <td>0.818137</td>\n      <td>1.093405</td>\n      <td>1.043078</td>\n      <td>0.536570</td>\n      <td>0.776659</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.942234</td>\n      <td>1.060991</td>\n      <td>0.901738</td>\n      <td>1.154064</td>\n      <td>0.997016</td>\n      <td>1.220186</td>\n      <td>0.963888</td>\n      <td>0.773595</td>\n      <td>0.889257</td>\n      <td>0.944109</td>\n      <td>...</td>\n      <td>0.981864</td>\n      <td>1.131911</td>\n      <td>0.968736</td>\n      <td>0.913845</td>\n      <td>1.054825</td>\n      <td>0.845073</td>\n      <td>1.068571</td>\n      <td>1.021406</td>\n      <td>0.520027</td>\n      <td>0.762446</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.921719</td>\n      <td>1.047234</td>\n      <td>0.912163</td>\n      <td>1.169759</td>\n      <td>1.043633</td>\n      <td>1.209286</td>\n      <td>0.958797</td>\n      <td>0.789999</td>\n      <td>0.870221</td>\n      <td>0.980002</td>\n      <td>...</td>\n      <td>0.988514</td>\n      <td>1.109596</td>\n      <td>0.929606</td>\n      <td>0.898687</td>\n      <td>1.030213</td>\n      <td>0.820845</td>\n      <td>1.071640</td>\n      <td>1.009794</td>\n      <td>0.550181</td>\n      <td>0.746763</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6   \\\n0   1.000000  0.131174  0.549706 -0.441372 -0.156160 -0.350425 -0.365436   \n1   0.131174  1.000000 -0.329388  0.522053  0.351508  0.055958 -0.540975   \n2   0.549706 -0.329388  1.000000 -0.640719 -0.539249 -0.092233  0.128965   \n3  -0.441372  0.522053 -0.640719  1.000000  0.772589  0.441081 -0.383995   \n4  -0.156160  0.351508 -0.539249  0.772589  1.000000  0.201630 -0.461348   \n..       ...       ...       ...       ...       ...       ...       ...   \n95  0.257535  0.230811 -0.036923 -0.302100 -0.327416 -0.696337  0.157545   \n96 -0.013536  0.236138 -0.267016  0.539632  0.659723  0.572410 -0.597255   \n97 -0.293862 -0.350629  0.203650 -0.206051 -0.254270  0.431614  0.008955   \n98 -0.135694  0.309295 -0.520956  0.157907  0.096842 -0.542866  0.209487   \n99  0.587804 -0.051118  0.287001 -0.573632 -0.357268 -0.289068  0.117927   \n\n          7         8         9   ...        90        91        92        93  \\\n0  -0.098604  0.081547 -0.000182  ... -0.650736  0.348055  0.190586  0.696762   \n1  -0.479354 -0.632625 -0.385677  ...  0.156758  0.516056 -0.211925 -0.069206   \n2   0.051375  0.182751  0.040472  ... -0.597711  0.142684  0.114491  0.481206   \n3  -0.449311 -0.616200 -0.108167  ...  0.417489  0.044653 -0.577060 -0.436045   \n4  -0.395026 -0.558801 -0.085330  ...  0.209730  0.046998 -0.427101 -0.032089   \n..       ...       ...       ...  ...       ...       ...       ...       ...   \n95  0.517110  0.263671 -0.198884  ...  0.124277  0.379473  0.450632  0.125674   \n96 -0.717425 -0.462374  0.152776  ... -0.112933 -0.129984 -0.454427  0.022258   \n97 -0.123308  0.166170  0.053032  ... -0.157425 -0.476429 -0.177707 -0.177191   \n98  0.383728  0.013005 -0.070236  ...  0.340258  0.139302  0.354510 -0.265745   \n99  0.037235  0.223253  0.101603  ... -0.392520  0.163682  0.534801  0.605016   \n\n          94        95        96        97        98        99  \n0   0.816744  0.257535 -0.013536 -0.293862 -0.135694  0.587804  \n1   0.224943  0.230811  0.236138 -0.350629  0.309295 -0.051118  \n2   0.328643 -0.036923 -0.267016  0.203650 -0.520956  0.287001  \n3  -0.161592 -0.302100  0.539632 -0.206051  0.157907 -0.573632  \n4   0.068313 -0.327416  0.659723 -0.254270  0.096842 -0.357268  \n..       ...       ...       ...       ...       ...       ...  \n95  0.395404  1.000000 -0.748179 -0.442109  0.681588  0.231212  \n96 -0.065174 -0.748179  1.000000  0.120370 -0.442087 -0.116700  \n97 -0.546106 -0.442109  0.120370  1.000000 -0.520502 -0.239875  \n98  0.130215  0.681588 -0.442087 -0.520502  1.000000  0.013680  \n99  0.439321  0.231212 -0.116700 -0.239875  0.013680  1.000000  \n\n[100 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.131174</td>\n      <td>0.549706</td>\n      <td>-0.441372</td>\n      <td>-0.156160</td>\n      <td>-0.350425</td>\n      <td>-0.365436</td>\n      <td>-0.098604</td>\n      <td>0.081547</td>\n      <td>-0.000182</td>\n      <td>...</td>\n      <td>-0.650736</td>\n      <td>0.348055</td>\n      <td>0.190586</td>\n      <td>0.696762</td>\n      <td>0.816744</td>\n      <td>0.257535</td>\n      <td>-0.013536</td>\n      <td>-0.293862</td>\n      <td>-0.135694</td>\n      <td>0.587804</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.131174</td>\n      <td>1.000000</td>\n      <td>-0.329388</td>\n      <td>0.522053</td>\n      <td>0.351508</td>\n      <td>0.055958</td>\n      <td>-0.540975</td>\n      <td>-0.479354</td>\n      <td>-0.632625</td>\n      <td>-0.385677</td>\n      <td>...</td>\n      <td>0.156758</td>\n      <td>0.516056</td>\n      <td>-0.211925</td>\n      <td>-0.069206</td>\n      <td>0.224943</td>\n      <td>0.230811</td>\n      <td>0.236138</td>\n      <td>-0.350629</td>\n      <td>0.309295</td>\n      <td>-0.051118</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.549706</td>\n      <td>-0.329388</td>\n      <td>1.000000</td>\n      <td>-0.640719</td>\n      <td>-0.539249</td>\n      <td>-0.092233</td>\n      <td>0.128965</td>\n      <td>0.051375</td>\n      <td>0.182751</td>\n      <td>0.040472</td>\n      <td>...</td>\n      <td>-0.597711</td>\n      <td>0.142684</td>\n      <td>0.114491</td>\n      <td>0.481206</td>\n      <td>0.328643</td>\n      <td>-0.036923</td>\n      <td>-0.267016</td>\n      <td>0.203650</td>\n      <td>-0.520956</td>\n      <td>0.287001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.441372</td>\n      <td>0.522053</td>\n      <td>-0.640719</td>\n      <td>1.000000</td>\n      <td>0.772589</td>\n      <td>0.441081</td>\n      <td>-0.383995</td>\n      <td>-0.449311</td>\n      <td>-0.616200</td>\n      <td>-0.108167</td>\n      <td>...</td>\n      <td>0.417489</td>\n      <td>0.044653</td>\n      <td>-0.577060</td>\n      <td>-0.436045</td>\n      <td>-0.161592</td>\n      <td>-0.302100</td>\n      <td>0.539632</td>\n      <td>-0.206051</td>\n      <td>0.157907</td>\n      <td>-0.573632</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.156160</td>\n      <td>0.351508</td>\n      <td>-0.539249</td>\n      <td>0.772589</td>\n      <td>1.000000</td>\n      <td>0.201630</td>\n      <td>-0.461348</td>\n      <td>-0.395026</td>\n      <td>-0.558801</td>\n      <td>-0.085330</td>\n      <td>...</td>\n      <td>0.209730</td>\n      <td>0.046998</td>\n      <td>-0.427101</td>\n      <td>-0.032089</td>\n      <td>0.068313</td>\n      <td>-0.327416</td>\n      <td>0.659723</td>\n      <td>-0.254270</td>\n      <td>0.096842</td>\n      <td>-0.357268</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0.257535</td>\n      <td>0.230811</td>\n      <td>-0.036923</td>\n      <td>-0.302100</td>\n      <td>-0.327416</td>\n      <td>-0.696337</td>\n      <td>0.157545</td>\n      <td>0.517110</td>\n      <td>0.263671</td>\n      <td>-0.198884</td>\n      <td>...</td>\n      <td>0.124277</td>\n      <td>0.379473</td>\n      <td>0.450632</td>\n      <td>0.125674</td>\n      <td>0.395404</td>\n      <td>1.000000</td>\n      <td>-0.748179</td>\n      <td>-0.442109</td>\n      <td>0.681588</td>\n      <td>0.231212</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>-0.013536</td>\n      <td>0.236138</td>\n      <td>-0.267016</td>\n      <td>0.539632</td>\n      <td>0.659723</td>\n      <td>0.572410</td>\n      <td>-0.597255</td>\n      <td>-0.717425</td>\n      <td>-0.462374</td>\n      <td>0.152776</td>\n      <td>...</td>\n      <td>-0.112933</td>\n      <td>-0.129984</td>\n      <td>-0.454427</td>\n      <td>0.022258</td>\n      <td>-0.065174</td>\n      <td>-0.748179</td>\n      <td>1.000000</td>\n      <td>0.120370</td>\n      <td>-0.442087</td>\n      <td>-0.116700</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>-0.293862</td>\n      <td>-0.350629</td>\n      <td>0.203650</td>\n      <td>-0.206051</td>\n      <td>-0.254270</td>\n      <td>0.431614</td>\n      <td>0.008955</td>\n      <td>-0.123308</td>\n      <td>0.166170</td>\n      <td>0.053032</td>\n      <td>...</td>\n      <td>-0.157425</td>\n      <td>-0.476429</td>\n      <td>-0.177707</td>\n      <td>-0.177191</td>\n      <td>-0.546106</td>\n      <td>-0.442109</td>\n      <td>0.120370</td>\n      <td>1.000000</td>\n      <td>-0.520502</td>\n      <td>-0.239875</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-0.135694</td>\n      <td>0.309295</td>\n      <td>-0.520956</td>\n      <td>0.157907</td>\n      <td>0.096842</td>\n      <td>-0.542866</td>\n      <td>0.209487</td>\n      <td>0.383728</td>\n      <td>0.013005</td>\n      <td>-0.070236</td>\n      <td>...</td>\n      <td>0.340258</td>\n      <td>0.139302</td>\n      <td>0.354510</td>\n      <td>-0.265745</td>\n      <td>0.130215</td>\n      <td>0.681588</td>\n      <td>-0.442087</td>\n      <td>-0.520502</td>\n      <td>1.000000</td>\n      <td>0.013680</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>0.587804</td>\n      <td>-0.051118</td>\n      <td>0.287001</td>\n      <td>-0.573632</td>\n      <td>-0.357268</td>\n      <td>-0.289068</td>\n      <td>0.117927</td>\n      <td>0.037235</td>\n      <td>0.223253</td>\n      <td>0.101603</td>\n      <td>...</td>\n      <td>-0.392520</td>\n      <td>0.163682</td>\n      <td>0.534801</td>\n      <td>0.605016</td>\n      <td>0.439321</td>\n      <td>0.231212</td>\n      <td>-0.116700</td>\n      <td>-0.239875</td>\n      <td>0.013680</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 100 columns</p>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "links = corr.stack().reset_index()\n",
    "links.columns = ['var1', 'var2','value']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "links_filtered=links.loc[ (links['value'] > 0.8) & (links['var1'] != links['var2']) ]\n",
    "links_filtered=links_filtered[links_filtered.var1.isnull() == False]\n",
    "links_filtered=links_filtered[links_filtered.var2.isnull() == False]\n",
    "\n",
    "links_filtered = links_filtered.sort_values(by=['value'],ascending=False)\n",
    "links_filtered = links_filtered.groupby('var1').head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArJ0lEQVR4nO3deXxU1f3/8dedJXsmQxYoIKgYBARBWbXaulDZRBZxt2oXC8afSLVKbbXiLrYuBRQEbavVWqiA0rK4IGq/FFdcQNkSVAQCQpbJwmSSzMz9/XGTyUICCWa/7+fj4QNz5947Z3xI3nPuOedzDNM0TURERGzC0doNEBERaUkKPhERsRUFn4iI2IqCT0REbEXBJyIitqLgExERW1HwiYiIrSj4RETEVhR8IiJiKwo+ERGxFQWfiIjYioJPRERsRcEnIiK2ouATERFbUfCJiIitKPhERMRWFHwiImIrCj4REbEVBZ+IiNiKgk9ERGxFwSciIrai4BMREVtR8ImIiK0o+ERExFZcrd0AERFbME0o2QdluRCVArFdwTBau1W2pOATEWlOoQBkLYLM+VC4veq4pw/0vhHSp4IzpvXaZ0OGaZpmazdCRKRD8mfDO2PBt6n+c7wD4dw1ENet5dplc+rxiYg0h1CA8rVjuPWZ3ezJT6EsaDBjZBGvfBrHwSJresXufCeDe+5mAeNg9Pvq+bUQBZ+ISHPIWsTyt7PoFOdm3pU+8v0Go55I46M7DwDg8xtc+nQq90woAN/n1uPQPje3cqPtQbM6RUSammlC5nzGDwwwc3RR5JCr2m/cx95I5BdnH6KLJ2wdyFzQCg21JwWfiEhTK9kHhduJjzZJiDEpDhhMfSGZmWOsEMwpdrA+K5rLhvqrrincZo0JSrPTo04RkaZQfblCyYHI4b0+B9c/n8x1Zx5i8uklAKzcFMOk00tw1u56lOVpkksLUPCJiHwf9S1XAA4WObjqmRQemFTAj3qXRY6vz4xmxsjiw+8VldzcrRUUfCIix+4oyxXmrUugwO9gztpE5qy1jr1wfS47D7romRKsebKnr3p7LUTr+EREjkUoQGjNcG5/djc7D7gwDJPZUwoIhuCO5V5cDpNeaSEevcSHoyGzKYbM0azOFqIen4jIschaxJsbMoEYVtyUw4adUTyyJhGHAbf8pIiR/Uq56SUva7dFM+qU0iPfyzsI0qe1SLNFwSci0ngVyxXGDAjwk34BAPbkO/HEmpyQEsTnd2CaUFzqwH203p53EJy3BpzRzd9uARR8IiKNV7FcAcDlhBmLvbz2RQyLrskn3+/gzleSmPNWAp4YkzNPqqe35+kLvTOsnp5Cr0VpjE9EpLF8m2H1wBqHDhQ6GD8vlZJyg6U35NLnB0Ge+18cO75z89DFBdZJ578FMZ2t2ZuayNJqtIBdRKSxolIAWLoxlnnrEgCIjTJxGOCNM0mIsaqxdPGEKSiptvWQpy94Byj0Wpl6fCIijWWasKof/pwd3LLEy8EiB+Uhg5vOK6ZTfJgHV3lwOU3cTvjTJT56JIes0Bu/tbVbLij4RESOzfa5sHFGw8/XcoU2Q486RUSORfpUay+9htByhTZFwSciciycMdYGst5BRz5PyxXaHD3qFBH5PiK1OhdYOyxU0nKFNkvBJyLSVPzZ1g4LWq7Qpin4RETEVjTGJyIitqLgExERW1HwiYiIrSj4RETEVhR8IiJiKwo+ERGxFQWfiIjYioJPRERsRcEnIiK2ouATERFbUfCJiIitKPhERMRWFHwiImIrCj4REbEVBZ+IiNiKgk9ERGxFwSciIrai4BMREVtxtXYDRKQa04SSfVCWC1EpENsVDKO1WyXSoSj4RNqCUACyFkHmfCjcXnXc0wd63wjpU8EZ03rtE+lADNM0zdZuhIit+bPhnbHg21T/Od6BcO4aiOvWcu0S6aAUfCKtKRSA10eQs+cLxvw5jcVTc3nsjUQOFlnD77vznQzuWc6Cn+aDdxCMfl89P5HvSY86Rapr6TG2rEWU527it0s7EeO2voMu+Gk+AD6/waVPp3LPhALrXN/n1uPQPjc3X3tEbECzOkXA6nltnwur+sGr3WH1QOvPVf2s46FA07+naULmfO5f6eGaM/38wBOq8fJjbyTyi7MP0cUTrjqYuaDp2yFiMwo+EX82vD4CNs6oObEErJ83zrBe92c37fuW7GPJW9+SHB/m3D6lNV7KKXawPiuay4b6a7VnW9O3Q+RYmab1/6Nvs/VnOxk50xif2FsoQGjNcG5/djc7D7gwDJPZUwro+4MgAK98Gstf18fzn+k5TT/G5tvMxePPwzDAAL7MdtMrLcjffpbH6i9iKChxMGNk8eHXjdsM3gFN0waRY9HOZyGrxyf2lrWINzdkArDiphxmjinikTWJAHyx18U/P4wj8s2wcoytqUSlsPzGXJZl5LI0I5f+3cqZc4WPzp4w6zOjOb9WL7DquuSma4NIY7XWE5ImpOAT+6oYYxszIMAfp/gA2JPvxBNrknfIYPYaD/dWTiyp1JRjbLFdrW/Iddh50EXPlODhL3j6akmDtJ5QgHkzz+Gi+7MZ8+dU/vlhHDnFDn7+t05cPD+FiU+m8k2O01qa88645hkbbwIKPrGvkn2Rb6wuJ8xY7OUPryYx+fQSbnvZy6yLCkmIrjUS0JRjbIZhPRaqsDQjl/TOVti9fdtBkmLrGIXondE07y1yDDYsvYuPt+Wx4v/lsCwjl2yfkwdWepg8uITlN+Yyc0whWQcrFgs09ROSJqQxPrEv32Zr9mY1BwodnPlwF9ISQxzXKUQgaJD5nYvLh/m5b2KhdVJTjrFVrOM74uL1St5BMPoDcEY3zXuLNIZp8vD1vaAslx37XRSVOvjDhYXc+FInrj3jEOu2xdAjOch9EwuJi6qIFU9fGL+1ddtdB/X4xL6iUgBYujGWeesSAIiNMklLDPH27QdYmpHLgqvz6d0lWBV60LRjbM4YqyKLd9CRz/MOgvPWKPSkZdQ1W7NkH3l5+Wza7WbhNfnMvtjHTf/sxJ48J964MEum5dLdG+KptxOq7tNGZyFrAbvYV8UY27hTd3DLEi8Xz0+hPGRw74RCYt31XNMcY2xx3QhfsIFZV3flrstTiS79qub79c6A9GkKPWl+R5qtedwkOsWHOalzkCgXpHcOEe0yCZlwwSnWWN4FpwR45DVPzXuW5bW5cWkFn9hXxRhbXOEMFl6TX+cpPZJDrJyeU3WgmcbYPty4ieVfHsf9U760viGX5Vk9yzb2C0M6sCPVjC3cDlseYdgJ0fxlfQLTfnyI7wodlJQZjD4lwLptMVwypIT3v4rm5C61JmW1wVnIGuMTe2vEGFt5Qn/cF25slp7XzJkziY6O5v7772/ye4scVa2/B+UhuPVfXvbkOykLGswYWcSo/tbymgdWetiwM4qwCXeMLSK9c5DbXvbiLzPwxIR58qp8vHFte4xPPT6xt8oxtnfGWbPQ6rG/tAtXzg2y9KxiUlKaKPgqxk3M0hz+740lzP/rK01zX5HGylpU48vf8k9i6RQXZt6VPvL9BqOeSGNU/wMA3DW+8LDLF0/Nrfu+bXQWsia3iMR1syqyDJljfUOtztMXhsyhyzXfMOzHExgzZgwFBbXW9jW2bFOtuqDGmkG8N/NbTtt7VfPVBRWpT8V61urGDwwwc3RR5GXXsSSFd5A1Nt0G6VGnSG31jLGZpsn06dP57LPPeP3114mPcTa+bJP23pO2xp9tFWSvQ3HA4GfPJXP1CD+TTy9p+D0rZyHHdm2iRjYtBZ9II4TDYX75y19SkruTl6b5cBRsrv/kWgFWHiji1qsHsee7wsi4STdviD+sSMJpQJTLZM4VPtISw9p7T1pOHetZAfb6HFz/fDLXnXmIK4bXEXqn/Bb2rLCWLFRqJ7OQFXwijeQvyufGKX3JKQqTGB3mwckFGAbcscxLeQiiXDD/6jyS480aAbbkz9fz5Xsvc9/Ewsi4Sc/kEPdOKGBA9yAvvBfHzoMu7plQMYYyZI723pPmV0eP72CRg0ueTuGBSQX8qHdZ3ddN2mt9qWuHs5A1uUWkkZY89Ws6J5Ty3M8KyDrg5K5XkygPGdwxtpAhx5ezalMMXx10kRxfXlW26eTpjD/u/7iw1rjJ/KvzI/vthcIQ7a72PTRzgYJPml9lzdhqj+vnrUugwO9gztpE5qy1jr1wfW7V+tbq61njurWbwKuk4BNpDNNkx6Z3Oa9i54T0ziE273WTlhDmzS0xPLzaw8DjyrnzwqqZb2bmAvLjzye5bAfEWOMmU19IZuaYokjoffSNm79tiGd5RrXZcZVVL9rZLxVpZyprxm6cETl038TCmtWKamujszUbSrM6RRqjZB/9Uw+ydms0pgkbd7nJO+Rk+3duftS7lJdvyKWgxODlj2MjlxiF2/j5JWcC1rjJpQtTuGRw1WSBFZ/F8LtlXv7+izxSEsI1368sr8U+mthY+lRrTLoh2vBszYZS8Ik0RlkuVwzzkxBjMnl+Cq99EcOg48pIiA5zVnoZhgE/6VfK53uialy24tUVHCxycNUzKfx+XGFkssCyjbE8tyGelzNyOD4ldPj7tcGqF9IB2axmrB51ijRGVAqf7XZzdnop904o5PPdbvbkuzCBD76KYkSvMt7/Ooo+XcprXpfYh3nre1DgD0TGTUImbN/vprs3xK+etwLujF5l3FYxDqi996RFVa5nzVpkjS+3w9maDaVZnSKNYZrkLelDxoI8/GUGSbFhHr3UR+4hB3e+kkQwbNAzOcifL/cRVfm1srJs0/a5NcZRjkqzOqU1tcPZmg2l4BNprGMNMO29J9ImaIxPpLGOdSKAzcZRRNoqBZ9IY32fAKtWF3RXfmzN8yvqgjL6gzZb6kmkI9CjTpFjFdm0s+ZEgLKYk5j14j6mPfoJJ/TqU+/lPXr04L23lnFc57gOOY4i0lYp+ESaQq2JAA8//DDr169n5cqVGIZx2OmlpaV4PB78fj9Op7MVGixiX3rUKdIU4rqBd0Ck1/ab3/yGb775hmXLltV5+q5du+jRo4dCT6QVKPhEmkFUVBQLFy5kxowZh+/fB3z11Vf06tWrFVomIgo+kWZy9tlnc+GFF3LnnXce9pqCT6T1KPhEmtHs2bNZtmwZH374YY3jX3/9NSeeeGIrtUrE3hR8Is0oOTmZRx99lKlTpxIMBq39iPzZBHM+5ZQTvdbPItKiNKtTpJmZpsmFY0dy20QP5/fYVmPfMzx9rC1h0qdqt3WRFqLgE2lm5QXf8uvrfkx2TjFlQYMZI4vomhTmjuVJRLlM+ncr575rj8dx/mtayyfSAhR8Is0pFGDJ3YP4Mms/900sJN9vMOqJNFITwtw3sYBhJ5TzyGuJpKcFmTLyZKuqi3p+Is1KY3wizSlrEePTM5lZsdWQaYLLAft8ToadYG1dNOyEMj78Jgp8n1uVYESkWSn4RJqLaULmfOKjTRJiTIoDBlNfSGbmmCJ6poR4b6e1We2bW2Lwl1VUd8lc0IoNFrEHbUQr0lxK9kUmsuz1Obj++WSuO/MQk08v4dTuZcxakcQTaw1GnFhGlKsi+Aq3WeXPNNYn0mzU4xNpLmW5ABwscnDVMyn8flwhVwwvAeCtrTHMuyqff03LJd9v8OPepdWuy2uN1orYhnp8Is0lKgWAeesSKPA7mLM2kTlrrZemnlPM5QtTiY0y+eFJpYzsVy34opJbobEi9qFZnSLNxTRhVb+a6/aOxtMXxm9tvjaJiB51ijQbw7AWpzdG74zmaYuIRCj4RJpT+lTwDmzYud5BkD6tedsjIgo+kWbljIFz11ihdiTeQXDeGnBGt0y7RGxMwSfS3OK6wej38Z10Lzv219x4tjw2nd8tjePgkJUQ27WVGihiLwo+kZbgjOGNXX25/d0LYdJeGLcZJu3FPTmTgh9cx6OPz2vtForYhmZ1irSQm2++mR49enD77bfXOL57925OO+00tm3bRlpaWiu1TsQ+1OMTaSH/+9//OOussw473qNHDy6//HIee+yxVmiViP2oxyfSAoqKiujatSu5ublERx8+geXbb7/ltNNOY8eOHaSmprZCC0XsQz0+ke+rYld1fJutP+v4Lvn+++8zePDgOkMPoGfPnur1ibQQ9fhEjlUoYG0jlDn/qLuqz5o1i/Lych566KF6b/ftt99y+umns337dvX6RJqRenwix8KfDa+PgI0zDi9JVrjdOv76CMj9BHyb2frJW5xdx/hedT179uTSSy/l8ccfb8aGi4h6fCK1maa1pVBZrlVoOrarVX6sUigAr4/gk01beWiVh6UZueQUO7j95SQKShyEwgZzrsjnhNRQjduG4nvj7HtTjZ5gbbt27WLw4MHq9Yk0IwWfSKWGPrrcPpf5j97Jsk9iiY0yWTk9h18v9nJ+vwBjBwS45i/JfFfoJC7KZMbIIgYfX14zFKd24YTL3qh3z71p06aRkpLCQw8+eOQAFpFjouATAfBnM3rkGSS6/AD0SA7xxOU+AOa+lcDWfW4W3NQDzlkNb49k1fpd9Otazs2LO7Fyeg5nPdKZa884xD8/jMMwYNXNOZQGYdQTaZx1Uhnn9wswYVCA/2VFUVJu8JMz+8Lo9+vs+e36ejtP/2YwD1zXDeehrKoX6hg7FJHG0xifSChA4M0xmMEASzNyWZqRGwm9dduieWtrRcj4NsHbo6BwOxcODOCuVn1sT54Tb1yYVTfnMLp/gKfeTsA0weWAj3ZFsc/n5PKFKbzyaSw/PKkMfJ9bvcvaTSnazZzfjmXL/jim/DGfbftd5BQ7+PnfOnHx7Bwm/vx+vnlhqDXGKCLHRMEnkrWILVu3UVJucOWiZC59OoWNu9x8nePkxffj+M2owqpzC7fUeYtOcWEuOCVAfLTJ+IElfLLLzdQXkpk5pigSikum5dLdG+KptxOsizIX1LxJKMCbcy+AsnxW3JTDzDFFPLImkQdWepg8uITlN+Yyc0whWTuz4J1x1qNZEWk0BZ/Ym2lC5nxi3SY3nFPMS7/KY/YUH9Nf6sRvl3l5ZEoBLufRbzPsxDLWbbN6hq9/GcOWfW4uGexn8uklkVAEuOCUAJv2uK2LCrex9j9/Z//+/ZimCVmLGHPidv44xQfAnnwnnlizUT1GETk6BZ/YW8k+KNxOr7QgFw/2YxhwUloIh8N6fJnxYidmrUjif1lRPLkuod7bzLqokKUb4xg7J5Wn301g9hQfVwwvAWqG4vtfRXNyl2Dkultv+hndunXD7XaRufoWAFxOmLHYyx9eTeLi00sa3mOsrgGL6kXsSpNbxN58m2H1QJ7fEMe2/W4evriA/QUOLluYwrrfHMTlhA07o3jhvXgW/DT/qLe7e4WHf38WS3rnqnD78xU+bnvZi7/MwBMT5smr8vHGWX/tzIl7OFDs4suP3+T8gmtq3OtAoYPx81IpKTd49/YDJMebfLHXxSOveXjhl3nWSZP21pwd2ohF9SJ25WrtBoi0qqgUAK4c7ueWJV4mPZWCYcBjl/mO/IjT0w8Ktx52+L6Jhdw3sfCw44un5h52zO8+nrj47nSJhy5nDYLVsHRjLPsKnEw/v5jYKBOHAWdU9BgvGVJyWI+Rsryq4PNnwztjrUk4tVUuqt/5F2tj3HqWUojYgXp8Ym+mCav6HV595QgC0ScSM3a9NcHE9/kxv/UfXvWQm3o1Dz/8MEnuQ/Bqd/xlBrcs8XKwyEF5yOCm84rp37283h7juqQX+NGoy3E7QpSvGs6vF+5md74Lp8PkT5cUECiHO5Z7cTlMeqWFePQSH47kQfUupRCxAwWfyPa5Vm+ogX6/LB5Hv1/zq19ew0fPX8ZFJ24i2t3I9/QOIn/4a9xx5yxWrlzJ3DlzuDj6Loyihgfwjv0OhsyKwzAMnr19IEklG1n+aSwLr8nnvzuieeG9OMImXDXCz8h+pdz0kpcJp5Uw6pRSOPU+6DFJC+PFljS5RSR9KngHNuxc7yAyHv+M1atX069fP4Z12RUJvdF/TuWSBSlcsiCFW5Z4j3gPzltDp9QfsHDhQpYsWcIf7r6bZ99tXHqm/+Ru/vGPfzBw4KmcFr+BXmlBQmEIh6EoYOBywoDu5fj8DkwTiksduCv/xm++G1YPhFe7Wz3e7XO1PEJsQz0+EbCKSb95FoSP8MvfEcOWnn/h57fMwe128+C1XTgnYTkAgXKY8GQab9xysP7rozvDgDshfRo4a25PVFpayqN/fIDJsQ+zeW8UL38cZ903aLAl2828K/N5+t0E3E6TlIQwc6/wEZs2AMZ8CKV58Gp39voc/OK5ZA6VOsg/5OC5X+SyN9/Fna8kkZIQwhNjsjQjh5j68tU7UON/YgsKPpFQgPJVw7n1md3syXdSFjSYMbKI3l2C3LLEi2FAny5BHppcwJZ9LjamPMk1112PY03/yNjgJ9+6mbG4E8d5gwTDBneMLWTI8eU138fTF8YfPiGmug//chHDY1dGfv798iRO6VbOwv8msDwjh7TEMA+vTqSzJ8wvzz7Ed8f9ng++iWOC6y7u+beHaJfJ78YVsdfn4LKFqRSVGLx8Qy690oJMfCqVfT4n3bwhZowsYlT/UgBe+TSWv66P5z/Tc6zeqMb/pIPTrE6RrEUsfzuLTnFu5l3pI99vMOqJNPp3CzJzTBE/PKmM3y5L4vUvYxh7aoABQ0qh9LsaE2IqF8BfNdzPVzlOrnk2hf/OPFBzZmjhNv772mJcnp7Ex8dH/omLiyM+Pp5ASQnJRa9BrHX657vd7PjOxUMXF3DBKQHSEsMABMMG0S7r+2rRJ39izadjmTAakmLDkTJqneJMgiFIjDVJiAmz/JNYUuNDnJgS5IHJBYx6Io1R/Q/wxV4X//wwjsi338qF8X1ubt7/5iKtSMEn9lZRuWX8wAAXnhqIHHI5YPMeN2f2KgPg/D6lvLsjmrGnBsh+927uWPk3/n5F1W16pQU5ITUYWQDfKT7Md0UOunvDNd7u+WefYMteB4cOFRPvKCTaKCY7p5Sd2X66emHPvKoHMPPWJXDLBUUAdPFY91m9OYYNO6O4fbS1ZCK9czmXX34ZJYc+Y+qPd3Prv7xMnp9CedDgjrFFdPeGuPHFZAzDxO2EByf7Ip8v75DB7DUe7p1QwO1LvVWNzFxw7MF3tC2dRNoABZ/YW0XllviKIbfigBGpsXn/Sk/kd3Z8TJiigPVDt4QCbp5+IxycGrnN4g9rLoAvChh0SQzXfjf+8rd/QvbKigXm30aOB6KOZ9Hqg4C1O0RBicHOgy7OSi+LnLPov/Gs2hTLP67PqzFO98y8+wmfUcT5vU0WXnP4IvsVN+VE/r04YPCz55K5fXQRt73sZdZFhcS4a412FG6z1gQ2ZqxPC+elHVHwib2VVS0s3+tzcP3zyVx35iEmn17CA6s8kdcOBRx4YqsCYujQM+B/fSK/5BuyAD4Y14uZv/wJO/cUYRgms6e4CIbgDyuScBrFRLliuLLIeqT5wVdRnJ1eGrl2zlsJbN7jZvG0HGKrhd6Sj2LJz8tl/loHj7+WypZsN5/evZ+EaJMbXuzEVcP9nNe39LDPd2JqkK9zXPxueRKBoEHmdy7uXuGJLL73F+whrgHBV15ezq0zMtjz5ZuUlfqZMbKIbl4X1/01hRNTg0AO15z5Wyaeq4Xz0nZocovYmz8bXu3OwSIHlzydwgOTCvhRb6uXdd1fk5l2TnFkjO+HJ5Uy8bSKWZ+T9sLupY1a/7fk4wQ++MrF45f52LAzimf+G09hwMG9EwoY0D3IC+/FsfOgi3smFLLgnXhcTvjVjw5xsMjBsAe7MKB7OTEVY3sXDSrhuh/6CeHGiTWJpnIizNnppcxY3Il9BQ4emVLAeX1L6/x8lXbnOcn4h7WvYKWTboshNvkkhg4dyrBhwxg6dCiDBg0iJqZmr23JP1/kyzWzuG/MV5Gx0Vt+UkxhwOCGcw7V/A+giTPSRqjHJ/YW2xU8fZi3Yh8Ffgdz1iYyZ6310r0TC7h7RRIPBw16dwkyfmBF6Hn6Wj2X9KlWCbC6SoTVYsZ24/Kh2Uw53fq5cueF2VPyI+N3oTBEVzx2zDi3KjTSEsN8M3tfnfetDL3qE2G+zHbx6KW+qmLWWOOFtT/fC9fn1ug9Rnj6svWbz/niiy/4+OOP+eijj3j22WfZvn07ffv2ZejQoZFAHHvyXi4s+dr6jBVjh5v2uNl50MUbX8ZwYmqQeycUkhBjauKMtBnq8Yk0snILQ+ZU/fL2Zx+9dFnSQAgdguKdgLXzwmtfxLDomnzO6WM9hvzoGze3vexleUYuKQmHjw3WyRENYev665/vxM/POlRjTPDXi71MPK0k8qjzmD5fNSUlJWzatImPPvqIjz/+mI8//ohlP99Kn65mZOzw6hF+yoLQr2uQgceVM+ctK3DvvqiifmkDlnSINDf1+EQa0XPDO8hagF4prpv1+C5rkTUbsnBb1WuevtA7A7pfBP/uFTk85wpfZOeFd24/yJtbopn3ViJ//0Vew0PPc0pkU9y6JsIcs9qfr5rY2FhGjBjBiBEjrAMVj4lrj40WlBgkVYyHjh0Q4K5Xk6puciwTZ0SamEqWiThjrIkX3kFHPq+i1Fjtqis4Y6we0vit1tjfuM3Wn+O3WseDxYC188K8ij39KndeWL05huc2xPNyRg7Hp4Rq3veU31rhWZ2nr9UjO/PvkUO1J8Ics/o+X33KcjlY5OCqZ1L4/bjCyP6DVz+TwqffWs9Q12dGM7B7rYX8ZXnfv60i34N6fCLQsJ5bHaXG6rxP7d5MxdZH404NcMsSLxfPT6E8ZHDvhEJu/ZeXbt4Qv3o+GYAzepVx22hr7R4n3wynzbZ6SGV5EJVccwuiCjsPuuhZOzTrc+p9sOulY/98tT5XXWOHsy4q5J5/J+F2mqQlhvnjJb5a1yU3/D1EmoHG+ETqUlfYHKtj2ProqGNh3/eeTfH5muNzibQAPeoUqUtcN/AOaJqxKMOwFnE3Ru+M5r1nU3y+5vhcIi1APT6RlhAKwOsjGj6BZvQHR3/s2Bz3bKy20AaRRlKPT6QlfN8JNC11z8ZqC20QaST1+ERaUqSm5feYQNMS92ysttAGkQZS8Im0lqacQNOc92yPbRA5AgWfiIjYisb4RETEVhR8IiJiK+23cot2ehYRkWPQ/oJPOz2LiMj30L4mt/iz4Z2xR14s6x2onZ5FRKRe7Sf4qlWICIfhd68ksSXbTbTL5E+X+jgxtVqRXu30LCIi9Wg/k1uyFkV6eq99GUNpucF/pufwu3GF3PcfT81zK3d6FhERqaV9BJ9pWmN6FT78Oorz+gYAGHJ8OZv2RB1+TeaClmqdiIi0I+0j+Er21ZjIUlxqkBhT9YTW4TAJ1t6OrHKnZxERkWraR/CV5db4MSHapLi0aulC2DRwOeu6Tjs9i4hITe0j+Cp2sK407IQy1m21Jq5s3OWm3w/K67lOOz2LiEhN7WMdX2xXa51exePOsQMC/DczmglPpmKa8MTlvsOv8fTVkgYRETlM+wi+cCkk9Y8En8MBj0wpOPI12ulZRETq0PaDryGL1mvzDrL2/xIREamlbQdfKEDpm2O4ddFuduWlkhgd5sHJBfRKs6Zwzn0rga373Cz4aX7VNdrpWUREjqBtT27JWsRLb+wkPtpk5fQc7p9UwF2vJgGwbls0b22tVZmlx8Uw+gNrTFBERKQObTf4Khat7/jOzXl9SgFI7xwi84CLr3OcvPh+HL8ZVVjzmoIt6umJiMgRtd3gq1i03r9bOWu3RmOa1tKF/QVOfrfcyyNTCg5fu6dF6yIichRtd4yvYtH6FcP8ZB7wMHl+CsNOKOP4lBA5xQ4yXuxEQYmD7wodPLkugZvOL664Lk/LGEREpF5tN/gqFq1/ttvN2eml3DuhkM93u9mT74pMZtmwM4oX3ouvCj3QonURETmitht8FYvWe6VlkvGih7lvJZIUG+bRS331X6NF6yIichRtez++7XNh44yGnz9kDvS5ufnaIyIi7V7bndwCkD7V2lG9IbRoXUREGqBtB58zBs5dY4XakWjRuoiINFDbftRZKRSwdlTPXGAtWajk6WvV5EyfptATEZEGaR/BV50/21qyEJWsiSwiItJo7S/4REREvoe2PcYnIiLSxBR8IiJiKwo+ERGxFQWfiIjYioJPRERsRcEnIiK2ouATERFbUfCJiIitKPhERMRWFHwiImIrCj4REbEVBZ+IiNiKgk9ERGxFwSciIrai4BMREVtR8ImIiK0o+ERExFYUfCIiYisKPhERsRUFn4iI2IqCT0REbEXBJyIitqLgExERW1HwiYiIrSj4RETEVhR8IiJiKwo+ERGxFQWfiIjYioJPRERsRcEnIiK2ouATERFbUfCJiIitKPhERMRWFHwiImIrCj4REbEVBZ+IiNiKgk9ERGxFwSciIrai4BMREVtR8ImIiK0o+ERExFYUfCIiYisKPhERsRUFn4iI2IqrtRsgbZxpQsk+KMuFqBSI7QqG0dqtEhE5Zgo+qVsoAFmLIHM+FG6vOu7pA71vhPSp4IxpvfaJiBwjwzRNs7UbIW2MPxveGQu+TfWf4x0I566BuG4t1y4RkSag4JOaQgF4fQSj78kmMdr6X6NHcoifnnGIWSuScDrgnJMD3DqqGLyDYPT76vmJSLuiR51SU9YiAgc3YZppLM3IiRy+4PE0nrk2j+NTQlz712S+2OtiAJ9bj0P73NyKDRYRaRzN6pQqpgmZ89myz01JucGVi5K59OkU3v8qirIQnJAawjDgnJNL+b/MaOuazAWt22YRkUZSj0+qlOyDwu3Eul3ccE4xVw3381WOk2ueTcETG46clhAdZldexf86hdusMUGN9YlIO6HgkypluQD0SgtyQmoQw4CT0kIkxobx+aseDhSXOkiKCVe7Lk/BJyLthh51SpWoFAAWfxjHff9JAmB/gYOSMoO4KJNvcpyYJry7I5rhvcqqXZfcGq0VETkmmtUpVUwTVvWjLG87tyzxstfnxDDg9+MKcRgw699JhMPw45NLuWNskXWNpy+M39q67RYRaQQFn9S0fS5snNHw84fM0axOEWlXFHx2Vlc5snApxcv6kxD86ujXewfB6A/AGd38bRURaSKa3GJHRyhHlh0/iYmzfKy79yQSgzvrv4d3EJy3RqEnIu2Oenx2489m9MgzSHT5AasqyxOX+wiF4YYXO3HVcD+n9e1Cp4vehuyV1jq9wm1V13v6Qu8MSJ+m0BORdkk9PjsJBQi8OQYzGGDp9NzI4W9ynMxY3Il9BQ6uGg6d+Bb+72KrHFmfm611emV51uxNLVsQkXZOyxnsJGsRW7Zuq1GVZeMuN4fKDB691McPT6q2RMFXUY4MrLDzDlDoiUiHoOCzi4pyZLFukxvOKealX+Uxe4qP6S91ok+XIL27BA+/RuXIRKQD0qNOu6goR9YrjRpVWTrFh/muyEF3b/jwa1SOTEQ6IPX47KKiHFntqixFAYMuiXWEXuS6vJZonYhIi1GPzy4qypFdOdzPLUu8THoqBcOAxy7z4XIe6TqVIxORjkXLGeyiohxZjXV7R6NyZCLSAelRp10YBvS+sXHX9M5onraIiLQiBZ+dpE8F78CGnesdZC1SFxHpYBR8duKMgXPXWKF2JCpHJiIdmMb47ChSq1PlyETEfhR8dqdyZCJiMwo+kY6qrm2nDKO1WyXS6rSOT6SjOcK2U/S+0Zrk5IxpvfaJtDL1+EQ6En82vDMWfJvqP8c70JrkpEfbYlPq8Yl0FKFAJPQ++dbNQ6s8LM3IJePFThwssiZw7853MrjnbhYwztp2Sj0/sSEFn0hHkbUIfJuY/3YCyz6JJTbKepiz4Kf5APj8Bpc+nco9Ewqqtp3qc3NrtlikVWgdn0hHULHtFMDxKUGeufbw4uKPvZHIL84+RBdPRVFybTslNqXgE+kIKradArhwYAB3rcLjOcUO1mdFc9lQf9XBym2nRGxGwSfSEVRsO1WflZtimHR6Cc7af+O17ZTYkIJPpCOo2HaqPuszozm/T2kd12nbKbEfBZ9IRxDb1VqnV4+dB130TAnWPOjpqyUNYktaxyfSUWyfCxtnNPz8IXM0q1NsST0+kY5C206JNIiCT6SjaOi2U4l94Iy/gSOqZdol0sboUadIR1PPtlNlQYMoV7W/7qrdKTal4BPpyHI/gQ1XQtGO+s9R7U6xGQWfSEcVClC6cji3LtrNrjwXidFhHpxcwMEiJ/ev9IABZ/Yq5c4Li6zHo6rdKTahWp0iHVXWIl56Yyfx0W5WTs8h64CTu15NIu+Qg0XX5tMzOcQlT6fwxV4XA1DtTrEPTW4R6Ygqanfu+M7NeRUL19M7h8g84GLl9Bx6Joc4VGpQFDCIqyhmrdqdYhcKPpGOqKJ2Z/9u5azdGo1pwsZdbvYXODEM69/PfyyNzolhunpD1jWq3Sk2oeAT6YgqandeMcxPQozJ5PkpvPZFDAOPK8fpgCHHl/PB7w8woHs5T61LrHadandKx6fgE+mIKmp3frbbzdnppbz6/3IZPzBAj+QQk+en4PMbACREmziMavPbVLtTbECzOkU6ItOEVf3I25dJxoud8JcZJMWGefRSH5/vjmLe2wlEu0w6J1rH4qNNq3bn+K2t3XKRZqfgE+moVLtTpE561CnSUal2p0idFHwiHVVDa3d6B8F5a8AZ3TLtEmlletQp0tHVU7sTT1/onWH19BR6YiMKPhE78WdbSxaiklWbU2xLwSciIraiMT4REbEVBZ+IiNiKgk9ERGxFwSciIrai4BMREVtR8ImIiK0o+ERExFYUfCIiYisKPhERsRUFn4iI2IqCT0REbEXBJyIitqLgExERW1HwiYiIrSj4RETEVhR8IiJiKwo+ERGxFQWfiIjYioJPRERsRcEnIiK2ouATERFbUfCJiIitKPhERMRW/j/nQk+qlvTAsAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Build your graph\n",
    "G=nx.from_pandas_edgelist(links_filtered, 'var1', 'var2')\n",
    "# Plot the network:\n",
    "nx.draw(G, with_labels=True, node_color='orange', node_size=70, edge_color='black', linewidths=5, font_size=10,pos=nx.spring_layout(G))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 10: Show one student knowledge on all time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6954822 , 0.4564812 , 0.5423922 , 0.56853366, 0.6949309 ,\n",
       "       0.6992768 , 0.7193209 , 0.6763963 , 0.65572995, 0.6319698 ,\n",
       "       0.8694718 , 0.73072594, 0.5053858 , 0.5765126 , 0.7899821 ,\n",
       "       0.5898231 , 0.70796835, 0.8460827 , 0.8044082 , 0.7652495 ,\n",
       "       0.5721069 , 0.89752704, 0.5436952 , 0.5431869 , 0.5053491 ,\n",
       "       0.6651835 , 0.6141199 , 0.64489836, 0.38807914, 0.6135989 ,\n",
       "       0.7048147 , 0.62349415, 0.71506906, 0.8127527 , 0.73477566,\n",
       "       0.7161563 , 0.7144337 , 0.7131349 , 0.608767  , 0.7203267 ,\n",
       "       0.77488995, 0.5848844 , 0.67769283, 0.41115412, 0.66282177,\n",
       "       0.64058197, 0.7477688 , 0.3681304 , 0.50326353, 0.61882347,\n",
       "       0.64083636, 0.49462032, 0.71923256, 0.5843102 , 0.69597125,\n",
       "       0.7893062 , 0.75254095, 0.6161817 , 0.6144015 , 0.69273543,\n",
       "       0.61317885, 0.5187306 , 0.5100732 , 0.55892587, 0.41817904,\n",
       "       0.5333922 , 0.5783792 , 0.7516109 , 0.56144214, 0.44760984,\n",
       "       0.45410505, 0.7410005 , 0.7475744 , 0.7558595 , 0.31840685,\n",
       "       0.3830249 , 0.43021992, 0.7506543 , 0.7418759 , 0.6987144 ,\n",
       "       0.58345395, 0.77008915, 0.7269535 , 0.43256444, 0.19177175,\n",
       "       0.5836527 , 0.5619848 , 0.80492216, 0.7695004 , 0.6179855 ,\n",
       "       0.6658565 , 0.35899976, 0.49242046, 0.61644495, 0.555617  ,\n",
       "       0.54542303, 0.6899539 , 0.672143  , 0.66251314, 0.71103257,\n",
       "       0.552683  , 0.45832732, 0.33303702, 0.5383704 , 0.75522876,\n",
       "       0.6431537 , 0.45355892, 0.44743723, 0.6822367 , 0.4731666 ,\n",
       "       0.62784564, 0.5740246 , 0.5521402 , 0.4716396 , 0.29743707,\n",
       "       0.40836075, 0.6368333 , 0.5167704 , 0.48092696, 0.6755135 ,\n",
       "       0.50791085, 0.57512176, 0.6162844 ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#student_desired[0][0][3]\n",
    "average_performance_of_each_concept=np.mean(prediction_of_one_student,axis=0)\n",
    "average_performance_of_each_concept #average_performance of all time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 11: Prediction on Given Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_on_given_data_set(d_set,prediction_set,student_answer_set):\n",
    "    \"\"\"\n",
    "    This function makes prediction\n",
    "    on given data set\n",
    "    \"\"\"\n",
    "    for data_candidate in d_set:\n",
    "        prediction_set.append(student_model.predict(data_candidate[0])[0])\n",
    "        #set index 0 for 246,set 1 for 124\n",
    "        temp=data_candidate[0]\n",
    "        student_answer_set.append(temp[0])\n",
    "    return prediction_set,student_answer_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_set_prediction=[]\n",
    "data_set_student_answer=[]\n",
    "data_set_prediction,data_set_student_answer=prediction_on_given_data_set(test_set,data_set_prediction,data_set_student_answer)\n",
    "#data_set_prediction\n",
    "#data_set_student_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_set_student_answer[0][0]\n",
    "len(data_set_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 11: Extracting Concept Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_one_relation(nbr_skills1,prediction_test1,student_test1,rm1):\n",
    "    \"\"\"\n",
    "    This function implements formula-1\n",
    "    for one candidate\n",
    "    \"\"\"\n",
    "    for i in range (nbr_skills1):\n",
    "        for j in range (nbr_skills1) :\n",
    "            if i != j :\n",
    "                index1 = np.argwhere(student_test1==1)\n",
    "                ax = np.argwhere(index1[:,1]==i*2+1)\n",
    "                if len(ax) == 0:\n",
    "                    indices = 0\n",
    "                else :\n",
    "                    indices = ax[0][0]\n",
    "                rm1[i][j]+=(prediction_test1[indices][j]*student_test1[indices][i*2+1])/(np.sum(prediction_test1[indices])-prediction_test1[indices][j])\n",
    "            else :\n",
    "                rm1[i][j]=0\n",
    "    return rm1\n",
    "relation_matrix=np.zeros((nb_skills,nb_skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "formula_1=np.zeros((nb_skills,nb_skills))\n",
    "for index,candidate_pred in enumerate(data_set_prediction):\n",
    "    formula_1=calculate_one_relation(nb_skills,candidate_pred,data_set_student_answer[index],formula_1)\n",
    "    print(\"##\",index/len(data_set_prediction)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "formula_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def counter_nbr_of_student(nbr_skills1,student_test1,rm1,len1):\n",
    "    \"\"\"\n",
    "    This function finds how many student answered skill n at least\n",
    "    once correctly\n",
    "    \"\"\"\n",
    "    for i in range (len1):\n",
    "        for j in range (nbr_skills1) :\n",
    "                index1 = np.argwhere(student_test1[i]==1)\n",
    "                a_b = np.argwhere(index1[:,1]==j*2+1)\n",
    "                if len(a_b) != 0:\n",
    "                    rm1[j] =rm1[j]+1\n",
    "    return rm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "#Calculates how many candidates answered skill n  correctly\n",
    "counter_matrix=np.zeros(nb_skills)\n",
    "counter_nbr_of_student(nb_skills,data_set_student_answer,counter_matrix,len(data_set_student_answer))\n",
    "counter_matrix[counter_matrix==0]=-1\n",
    "print(len(counter_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138.]\n",
      " [219.]\n",
      " [103.]\n",
      " [ 90.]\n",
      " [105.]\n",
      " [162.]\n",
      " [262.]\n",
      " [233.]\n",
      " [179.]\n",
      " [199.]\n",
      " [197.]\n",
      " [212.]\n",
      " [102.]\n",
      " [104.]\n",
      " [190.]\n",
      " [103.]\n",
      " [106.]\n",
      " [169.]\n",
      " [155.]\n",
      " [ 48.]\n",
      " [135.]\n",
      " [125.]\n",
      " [144.]\n",
      " [ 91.]\n",
      " [198.]\n",
      " [105.]\n",
      " [108.]\n",
      " [110.]\n",
      " [ -1.]\n",
      " [123.]\n",
      " [372.]\n",
      " [197.]\n",
      " [391.]\n",
      " [375.]\n",
      " [146.]\n",
      " [ 60.]\n",
      " [107.]\n",
      " [324.]\n",
      " [257.]\n",
      " [ 77.]\n",
      " [153.]\n",
      " [113.]\n",
      " [264.]\n",
      " [ 23.]\n",
      " [365.]\n",
      " [314.]\n",
      " [121.]\n",
      " [189.]\n",
      " [270.]\n",
      " [239.]\n",
      " [ 36.]\n",
      " [189.]\n",
      " [ 93.]\n",
      " [ 57.]\n",
      " [ 46.]\n",
      " [326.]\n",
      " [201.]\n",
      " [ 95.]\n",
      " [134.]\n",
      " [153.]\n",
      " [118.]\n",
      " [ 43.]\n",
      " [ 45.]\n",
      " [127.]\n",
      " [ -1.]\n",
      " [  6.]\n",
      " [102.]\n",
      " [108.]\n",
      " [ 58.]\n",
      " [  9.]\n",
      " [ 63.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [ 10.]\n",
      " [ 25.]\n",
      " [ 18.]\n",
      " [ -1.]\n",
      " [163.]\n",
      " [283.]\n",
      " [254.]\n",
      " [283.]\n",
      " [286.]\n",
      " [ 31.]\n",
      " [  8.]\n",
      " [ 42.]\n",
      " [ 96.]\n",
      " [ 65.]\n",
      " [146.]\n",
      " [142.]\n",
      " [116.]\n",
      " [ 56.]\n",
      " [113.]\n",
      " [170.]\n",
      " [202.]\n",
      " [ 95.]\n",
      " [241.]\n",
      " [223.]\n",
      " [306.]\n",
      " [205.]\n",
      " [  1.]\n",
      " [ 85.]\n",
      " [  2.]\n",
      " [ 94.]\n",
      " [112.]\n",
      " [ -1.]\n",
      " [125.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [  1.]\n",
      " [ 11.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [ -1.]\n",
      " [  1.]\n",
      " [ -1.]\n",
      " [ 10.]\n",
      " [ -1.]\n",
      " [148.]\n",
      " [ -1.]\n",
      " [ 89.]\n",
      " [ -1.]]\n"
     ]
    }
   ],
   "source": [
    "counter_matrix=counter_matrix.reshape(123,1)\n",
    "print(counter_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalizeRelation_formula_1=formula_1/counter_matrix\n",
    "print(normalizeRelation_formula_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"test_set_relation_prediction_calculate_one_relation_normalized.csv\",normalizeRelation_formula_1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_one_relation_improved(nbr_skills1,prediction_test1,student_test1,rm1):\n",
    "    \"\"\"\n",
    "    This function implements formula-2\n",
    "    for one candidate\n",
    "    \"\"\"\n",
    "    for i in range (nbr_skills1):\n",
    "        for j in range (nbr_skills1) :\n",
    "                index1 = np.argwhere(student_test1==1)\n",
    "                a_b = np.argwhere(index1[:,1]==i*2+1)\n",
    "                if len(a_b) == 0:\n",
    "                    rm1[i][j] = 0\n",
    "                else :\n",
    "                    indices = a_b[0][0]\n",
    "                    rm1[i][j]=(prediction_test1[indices][j]*student_test1[indices][i*2+1])\n",
    "    return rm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57752627 0.50702435 0.52225411 0.52268434 0.59546483 0.54896468\n",
      " 0.586743   0.55154842 0.48287794 0.52175659 0.58043289 0.5448786\n",
      " 0.49741817 0.47684222 0.57405531 0.50415289 0.52226561 0.59185892\n",
      " 0.5404225  0.54112875 0.50622934 0.61287773 0.49089068 0.49104509\n",
      " 0.47129631 0.51525354 0.51451087 0.51050854 0.46550569 0.5109818\n",
      " 0.57178396 0.47519076 0.57143056 0.55650938 0.56387639 0.52393651\n",
      " 0.55326873 0.53036624 0.51955712 0.52596945 0.55826592 0.51962519\n",
      " 0.53099924 0.47402707 0.50858742 0.50649089 0.53567898 0.44523156\n",
      " 0.47990471 0.50533873 0.50973064 0.4856497  0.54767781 0.48032892\n",
      " 0.55084938 0.55192077 0.55055177 0.52110529 0.47258338 0.51867747\n",
      " 0.53761959 0.5022859  0.51190907 0.52404439 0.48849452 0.49280554\n",
      " 0.52299386 0.55650109 0.51343566 0.47208533 0.48461986 0.54646325\n",
      " 0.55066442 0.54809958 0.48412201 0.48118943 0.49260512 0.53277093\n",
      " 0.52443707 0.51879305 0.4895713  0.53921759 0.53724402 0.47422928\n",
      " 0.42151901 0.52214646 0.48378804 0.5499106  0.54778695 0.51955914\n",
      " 0.54314822 0.45534092 0.49392304 0.51185709 0.51480961 0.51377809\n",
      " 0.52278489 0.51703393 0.51164204 0.56297946 0.50175053 0.5040288\n",
      " 0.46501204 0.4972336  0.52136141 0.51943082 0.49403548 0.50282013\n",
      " 0.52523541 0.49592349 0.52422994 0.51188803 0.50238514 0.49603617\n",
      " 0.46613604 0.45942688 0.50855815 0.50409728 0.50992852 0.51725703\n",
      " 0.51081634 0.49708387 0.50227886 0.70917988 0.34240025 0.56190085\n",
      " 0.55563349 0.74273491 0.704844   0.66836941 0.64740908 0.37159789\n",
      " 0.37864903 0.83614409 0.74366546 0.45804733 0.44638821 0.66354012\n",
      " 0.43932515 0.47812068 0.80368173 0.59344488 0.48708475 0.43139979\n",
      " 0.80470741 0.42365831 0.44633237 0.21064892 0.51852506 0.49475488\n",
      " 0.52356964 0.46231613 0.50447959 0.63859469 0.34213686 0.71854484\n",
      " 0.70472461 0.65958315 0.54109269 0.63316911 0.63140726 0.59138697\n",
      " 0.56439167 0.7266531  0.53965753 0.54657388 0.37797859 0.39760369\n",
      " 0.54915583 0.62067842 0.25675368 0.32304555 0.36897609 0.50022042\n",
      " 0.36539018 0.61780876 0.36616725 0.67742801 0.64608467 0.69696617\n",
      " 0.45575806 0.34562567 0.53258842 0.51559186 0.45148528 0.54714936\n",
      " 0.56297106 0.58750367 0.4350591  0.56430095 0.62429279 0.53976107\n",
      " 0.47955182 0.51774102 0.64568883 0.62840039 0.66492462 0.34933764\n",
      " 0.40218627 0.38280258 0.66447288 0.57137042 0.49185708 0.46020466\n",
      " 0.63649541 0.5584352  0.31068671 0.20580104 0.59767658 0.5077166\n",
      " 0.55798876 0.6483838  0.71380836 0.7576896  0.40112352 0.49636716\n",
      " 0.40996632 0.36279315 0.55428803 0.54197872 0.52334863 0.5671612\n",
      " 0.7140215  0.37484932 0.43470609 0.31804404 0.47497544 0.52405453\n",
      " 0.57711244 0.42801121 0.5458014  0.70376074 0.41794407 0.61599612\n",
      " 0.54135108 0.52467799 0.42455393 0.36167818 0.2738106  0.68031871\n",
      " 0.54001325 0.53785497 0.52458459 0.48468938 0.53299874 0.59196085\n",
      " 0.7184431  0.36668962 0.55661231 0.57665062 0.73918557 0.69705391\n",
      " 0.6274879  0.59062541 0.37404445 0.40675604 0.81316495 0.76569229\n",
      " 0.49608028 0.46335033 0.67583716 0.41323537 0.45952418 0.77790296\n",
      " 0.57566941 0.49515823 0.406414   0.79711139 0.410918   0.41960627\n",
      " 0.2465232  0.50820595 0.49081439 0.50890726 0.46475005 0.50873286\n",
      " 0.58455873 0.33197662 0.7332083  0.72128665 0.65744001 0.51982439\n",
      " 0.62363446 0.63464993 0.51004893 0.54555386 0.69441664 0.50722063\n",
      " 0.54366881 0.37006536 0.38530147 0.52343953 0.61431926 0.32976764\n",
      " 0.31009907 0.36546201 0.51197678 0.32783598 0.6065203  0.38668108\n",
      " 0.67411339 0.64661694 0.73453069 0.47280508 0.38030359 0.53277493\n",
      " 0.47595567 0.44218057 0.55386072 0.54199284 0.57116246 0.44552961\n",
      " 0.57176286 0.65672582 0.53569615 0.50242054 0.52412021 0.60113513\n",
      " 0.60997152 0.65755951 0.36414683 0.40612504 0.38631997 0.65884393\n",
      " 0.59881508 0.54826331 0.44508302 0.64089763 0.56591767 0.29634398\n",
      " 0.18513674 0.55219239 0.5135349  0.53327203 0.61480039 0.72304261\n",
      " 0.80720234 0.40073878 0.49571466 0.44500616 0.38975781 0.55032551\n",
      " 0.58779925 0.543024   0.58925647 0.69408464 0.35226554 0.46262714\n",
      " 0.35355452 0.43871626 0.51742125 0.59824717 0.42493278 0.55440211\n",
      " 0.70935798 0.43165582 0.59602517 0.55046177 0.53850341 0.42721745\n",
      " 0.33962867 0.26311025 0.69812965 0.50482619 0.51288521 0.517492\n",
      " 0.46803543 0.52351266 0.54879236 0.66498554 0.44920239 0.58464557\n",
      " 0.53572518 0.75219142 0.63702953 0.58669835 0.5397507  0.35446501\n",
      " 0.40772781 0.65890169 0.70257092 0.5202769  0.44072977 0.61355507\n",
      " 0.37314415 0.40617368 0.63730335 0.48425472 0.42346305 0.3531639\n",
      " 0.69701397 0.42221174 0.40748391 0.27102041 0.47725958 0.46166107\n",
      " 0.45618957 0.43266979 0.476605   0.53032589 0.27978802 0.73842436\n",
      " 0.66880351 0.64990759 0.45230231 0.59898639 0.57576126 0.39674485\n",
      " 0.45345521 0.57606542 0.40398949 0.48244396 0.36438841 0.27379104\n",
      " 0.45731974 0.50633866 0.41162959 0.31828451 0.34163415 0.51180649\n",
      " 0.29973513 0.5990507  0.38500008 0.69565916 0.58546686 0.6803537\n",
      " 0.45268759 0.35053778 0.47273928 0.44052747 0.45511615 0.56788254\n",
      " 0.54902369 0.57960838 0.43726712 0.5736059  0.60992312 0.51040488\n",
      " 0.50840348 0.50729942 0.53179085 0.54009563 0.58636081 0.44774929\n",
      " 0.4111039  0.40692011 0.59570938 0.53773689 0.53607434 0.41451812\n",
      " 0.58416134 0.4961687  0.34056604 0.25876051 0.50910562 0.46960124\n",
      " 0.38115701 0.51645285 0.67384225 0.86326784 0.43913305 0.47777116\n",
      " 0.43245733 0.47063094 0.50315064 0.54116696 0.49343228 0.56578755\n",
      " 0.58525085 0.37344608 0.44850627 0.43999484 0.42059571 0.41317889\n",
      " 0.53898579 0.42821321 0.60363102 0.64556944 0.44333568 0.56315655\n",
      " 0.53099239 0.52657622 0.43585891 0.3487981  0.27168253 0.65132427\n",
      " 0.45789871 0.51542795 0.44293737 0.49096474 0.47461012 0.46349433\n",
      " 0.64220202 0.44197109 0.58739853 0.53227693 0.73063731 0.6534335\n",
      " 0.57451099 0.52764755 0.35112    0.37028906 0.633811   0.69585752\n",
      " 0.49743295 0.44638693 0.60151041 0.3547377  0.40157083 0.6221559\n",
      " 0.48093891 0.41598526 0.35164502 0.67010605 0.4193837  0.39052498\n",
      " 0.27710634 0.45778292 0.48207638 0.45342016 0.42320946 0.49490839\n",
      " 0.55567801 0.27828318 0.73139709 0.65287554 0.67903686 0.455376\n",
      " 0.57826269 0.58955753 0.39694941 0.44772562 0.57278764 0.37544075\n",
      " 0.44178548 0.33467692 0.27297759 0.44633418 0.4912703  0.39060459\n",
      " 0.32460737 0.3442896  0.49750087 0.31147885 0.58134449 0.38010088\n",
      " 0.69389671 0.56782895 0.6639356  0.42775559 0.31957322 0.46029702\n",
      " 0.40846261 0.47294328 0.54917914 0.55316263 0.59889555 0.44114104\n",
      " 0.54717594 0.56201571 0.50120145 0.51637965 0.48683199 0.5238843\n",
      " 0.52487594 0.56530321 0.44965321 0.40423834 0.41740772 0.57595223\n",
      " 0.51624519 0.52697998 0.40670019 0.55803323 0.4664064  0.36579317\n",
      " 0.2899285  0.51658344 0.47265095 0.35187268 0.47077847 0.7234326\n",
      " 0.86986566 0.44075871 0.49891052 0.42032367 0.47075334 0.51683176\n",
      " 0.51978517 0.50901693 0.5689829  0.5810914  0.38787985 0.41091561\n",
      " 0.45173803 0.44983634 0.396945   0.54336655 0.43224478 0.61411023\n",
      " 0.61834323 0.43378663 0.55697179 0.53912973 0.52671391 0.44854113\n",
      " 0.35460922 0.27800101 0.68043101 0.46118736 0.52454859 0.46855888\n",
      " 0.49345797 0.50210333 0.46961716]\n"
     ]
    }
   ],
   "source": [
    "# CALCULATING RELATION FOR ONE STUDENT\n",
    "one_student=np.zeros((nb_skills,nb_skills))\n",
    "one_student=calculate_one_relation_improved(nb_skills,data_set_prediction[0],data_set_student_answer[0],one_student)\n",
    "print(one_student[one_student!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17435614, 0.24060518, 0.18566979, 0.19195372, 0.16725534,\n",
       "       0.16936425, 0.192766  , 0.19305287, 0.24966477, 0.25022154,\n",
       "       0.1647808 , 0.15781393, 0.20144458, 0.20972105, 0.1834923 ,\n",
       "       0.2418469 , 0.23031088, 0.17240771, 0.20204746, 0.22901819,\n",
       "       0.2470795 , 0.17110807, 0.22652355, 0.22786393, 0.31917774,\n",
       "       0.20801288, 0.21053571, 0.20815035, 0.20703394, 0.20474425,\n",
       "       0.19847123, 0.2783165 , 0.16359282, 0.16842486, 0.17567096,\n",
       "       0.21020253, 0.18520563, 0.17907238, 0.21516539, 0.20731162,\n",
       "       0.17846299, 0.22150038, 0.20860547, 0.24674306, 0.27666765,\n",
       "       0.20400479, 0.19350568, 0.24276701, 0.2733034 , 0.26241812,\n",
       "       0.20137624, 0.27129907, 0.18550245, 0.24037139, 0.16733241,\n",
       "       0.18410135, 0.16551288, 0.22363962, 0.25290453, 0.2060634 ,\n",
       "       0.22606562, 0.21612886, 0.1875138 , 0.19187369, 0.17287774,\n",
       "       0.21884937, 0.18813815, 0.18491735, 0.19743735, 0.190446  ,\n",
       "       0.19226274, 0.19181133, 0.19294425, 0.18135495, 0.2310835 ,\n",
       "       0.22861061, 0.23614191, 0.17596269, 0.1908012 , 0.19786401,\n",
       "       0.22091797, 0.18224167, 0.20472897, 0.2652854 , 0.30967955,\n",
       "       0.19355213, 0.19768303, 0.23161922, 0.19576387, 0.15492187,\n",
       "       0.14140163, 0.21306537, 0.20056269, 0.23060671, 0.2330779 ,\n",
       "       0.19473285, 0.19265967, 0.19994693, 0.18254479, 0.17943981,\n",
       "       0.25211171, 0.22294426, 0.22925705, 0.21795516, 0.21970921,\n",
       "       0.18703785, 0.22380497, 0.17825666, 0.16401988, 0.22312305,\n",
       "       0.18352951, 0.19144425, 0.19183377, 0.22221776, 0.24915733,\n",
       "       0.29716532, 0.15799807, 0.20425147, 0.19607769, 0.20934547,\n",
       "       0.20866988, 0.19645187, 0.19497317, 0.21410259, 0.16248387,\n",
       "       0.19976485, 0.20405417, 0.20862086, 0.21745547, 0.21958318,\n",
       "       0.226606  , 0.19212909, 0.1815907 , 0.2373754 , 0.21538884,\n",
       "       0.18550017, 0.19632701, 0.21209542, 0.21074842, 0.21084366,\n",
       "       0.23411141, 0.22187091, 0.20614551, 0.21055683, 0.22466461,\n",
       "       0.1954989 , 0.2071155 , 0.14265855, 0.20933363, 0.20245164,\n",
       "       0.21347577, 0.20561538, 0.2021389 , 0.22166182, 0.2003876 ,\n",
       "       0.20570964, 0.21328148, 0.2054876 , 0.21708556, 0.21195213,\n",
       "       0.21318778, 0.24491245, 0.2224558 , 0.23229196, 0.23003956,\n",
       "       0.21472403, 0.19674741, 0.21629336, 0.22118941, 0.2242104 ,\n",
       "       0.13999754, 0.18397287, 0.19160616, 0.1976191 , 0.20411835,\n",
       "       0.20925631, 0.18324138, 0.20578341, 0.21551111, 0.20952957,\n",
       "       0.19559495, 0.1849627 , 0.21159003, 0.2168031 , 0.19426984,\n",
       "       0.20042242, 0.20612631, 0.20791699, 0.19320483, 0.20299767,\n",
       "       0.20744356, 0.20756056, 0.19345809, 0.20540286, 0.22664   ,\n",
       "       0.22018173, 0.22000997, 0.16674756, 0.19107662, 0.18350546,\n",
       "       0.21946099, 0.20787654, 0.18759082, 0.20766634, 0.21511907,\n",
       "       0.21280434, 0.17379916, 0.15119691, 0.22155005, 0.20746059,\n",
       "       0.23502169, 0.2317144 , 0.212843  , 0.19725471, 0.18769569,\n",
       "       0.20155515, 0.18470191, 0.16425308, 0.21008698, 0.19973308,\n",
       "       0.20238895, 0.20235304, 0.22758181, 0.18834839, 0.19228113,\n",
       "       0.15679989, 0.20819861, 0.22084413, 0.20780798, 0.19389506,\n",
       "       0.19349411, 0.21976955, 0.188039  , 0.21565626, 0.20246332,\n",
       "       0.2003462 , 0.19019464, 0.1933229 , 0.17710547, 0.21136038,\n",
       "       0.218804  , 0.20681597, 0.21231109, 0.19799695, 0.21064573,\n",
       "       0.22978566, 0.21689917, 0.17401023, 0.19788469, 0.21177262,\n",
       "       0.20762391, 0.21505211, 0.20615215, 0.20673059, 0.19339405,\n",
       "       0.19507013, 0.23085179, 0.2217685 , 0.20090276, 0.20378715,\n",
       "       0.21602607, 0.1982329 , 0.2026429 , 0.22660209, 0.21522521,\n",
       "       0.2095624 , 0.19836181, 0.22254389, 0.18961982, 0.19471355,\n",
       "       0.16695382, 0.2051677 , 0.20083921, 0.20749746, 0.20669787,\n",
       "       0.20384313, 0.20290546, 0.1944368 , 0.20990759, 0.21829391,\n",
       "       0.20481993, 0.20855275, 0.20876042, 0.21428264, 0.21122774,\n",
       "       0.21503085, 0.22198681, 0.2162127 , 0.21358276, 0.19262837,\n",
       "       0.20960104, 0.21083138, 0.22191325, 0.17980914, 0.17659991,\n",
       "       0.18978132, 0.20226361, 0.1831394 , 0.20543283, 0.19350713,\n",
       "       0.20477652, 0.21568865, 0.22082263, 0.20291092, 0.2035207 ,\n",
       "       0.21166412, 0.20013633, 0.19026611, 0.20288081, 0.19844534,\n",
       "       0.20213385, 0.19785466, 0.20568197, 0.21822059, 0.20599743,\n",
       "       0.20268367, 0.20793367, 0.21100143, 0.21372454, 0.217573  ,\n",
       "       0.17381636, 0.1929479 , 0.18519161, 0.21760187, 0.21786148,\n",
       "       0.20910376, 0.20084273, 0.2166069 , 0.21565571, 0.16577579,\n",
       "       0.13601536, 0.20468972, 0.20983804, 0.22461115, 0.21971262,\n",
       "       0.21559646, 0.21014471, 0.18751566, 0.2012902 , 0.2004884 ,\n",
       "       0.17646122, 0.2085851 , 0.21661913, 0.20999779, 0.21023624,\n",
       "       0.22122728, 0.17700084, 0.2046313 , 0.17430703, 0.19230493,\n",
       "       0.21804877, 0.21541823, 0.19250048, 0.19654318, 0.22151745,\n",
       "       0.19420811, 0.20866455, 0.20587068, 0.20562538, 0.19138787,\n",
       "       0.18153707, 0.1701843 , 0.21689385, 0.20454681, 0.1972146 ,\n",
       "       0.20944056, 0.19119377, 0.20689675, 0.21302864, 0.20076024,\n",
       "       0.21316614, 0.20785097, 0.19674292, 0.21127702, 0.19653364,\n",
       "       0.19275133, 0.18892344, 0.18327079, 0.19553616, 0.18705754,\n",
       "       0.20348657, 0.21070192, 0.19383835, 0.19611809, 0.17900077,\n",
       "       0.17911617, 0.18564561, 0.18104805, 0.17921934, 0.1723716 ,\n",
       "       0.1945979 , 0.19483137, 0.18908831, 0.18354415, 0.19267435,\n",
       "       0.18890979, 0.1860028 , 0.19243015, 0.19096988, 0.18408077,\n",
       "       0.16387024, 0.21140088, 0.20241014, 0.20247326, 0.181463  ,\n",
       "       0.20050952, 0.19439952, 0.16430486, 0.17873003, 0.18415303,\n",
       "       0.17220841, 0.1895303 , 0.18967337, 0.14894023, 0.1841996 ,\n",
       "       0.18290694, 0.2244452 , 0.18126148, 0.17740772, 0.20219634,\n",
       "       0.16744139, 0.20290282, 0.19266591, 0.21132152, 0.19529114,\n",
       "       0.20453535, 0.19427721, 0.18759143, 0.18781279, 0.185239  ,\n",
       "       0.19583217, 0.20801704, 0.20101961, 0.20512285, 0.19418539,\n",
       "       0.20634497, 0.20266872, 0.19627189, 0.20509727, 0.20126038,\n",
       "       0.18666124, 0.18924111, 0.19401481, 0.21372189, 0.19531334,\n",
       "       0.19506677, 0.1967499 , 0.19563996, 0.20445497, 0.18705039,\n",
       "       0.1974315 , 0.18907629, 0.19051375, 0.19010492, 0.18871808,\n",
       "       0.19188609, 0.16054117, 0.18456593, 0.20092592, 0.22474064,\n",
       "       0.2054813 , 0.19400405, 0.19483478, 0.21307619, 0.19070482,\n",
       "       0.19943393, 0.19081972, 0.20186295, 0.18653843, 0.18764331,\n",
       "       0.19838529, 0.21692324, 0.18436204, 0.17411954, 0.19407925,\n",
       "       0.19398656, 0.21399551, 0.20159764, 0.19946305, 0.19715746,\n",
       "       0.19858921, 0.20107104, 0.19525913, 0.18643828, 0.175729  ,\n",
       "       0.20235243, 0.18553261, 0.19819233, 0.17926664, 0.20056045,\n",
       "       0.18757004, 0.1799179 , 0.19388186, 0.20973458, 0.20882969,\n",
       "       0.19547656, 0.20522286, 0.20159453, 0.18874735, 0.1846871 ,\n",
       "       0.1815413 , 0.17758147, 0.17993447, 0.20154216, 0.20145057,\n",
       "       0.19632644, 0.19226811, 0.17017102, 0.17708639, 0.18123318,\n",
       "       0.17980837, 0.17605457, 0.17163027, 0.18708553, 0.19352636,\n",
       "       0.18121871, 0.18766575, 0.18481143, 0.19726365, 0.18487363,\n",
       "       0.18822266, 0.19830383, 0.19288071, 0.16298886, 0.20938907,\n",
       "       0.19758961, 0.21154824, 0.18269616, 0.19357231, 0.19905768,\n",
       "       0.16438957, 0.1764717 , 0.18310521, 0.16003895, 0.17355744,\n",
       "       0.17420779, 0.14849772, 0.17977483, 0.17746373, 0.2129811 ,\n",
       "       0.18486232, 0.17878667, 0.1965447 , 0.1740018 , 0.1969056 ,\n",
       "       0.19021419, 0.21078613, 0.18940775, 0.19959956, 0.1835773 ,\n",
       "       0.17102065, 0.18286965, 0.17175594, 0.20350301, 0.20116593,\n",
       "       0.20253505, 0.21194856, 0.19590575, 0.19683724, 0.18674978,\n",
       "       0.19273278, 0.20831497, 0.19314035, 0.183886  , 0.18390837,\n",
       "       0.18704728, 0.21463068, 0.19205154, 0.20009426, 0.19022454,\n",
       "       0.18782082, 0.20098645, 0.18352257, 0.18860087, 0.17773469,\n",
       "       0.20462589, 0.21300326, 0.19149   , 0.19313224, 0.14820677,\n",
       "       0.16824317, 0.21571275, 0.2264583 , 0.20624198, 0.20258791,\n",
       "       0.18936821, 0.21313161, 0.19589025, 0.19155419, 0.1968466 ,\n",
       "       0.20300299, 0.18521267, 0.19489576, 0.18175802, 0.22271279,\n",
       "       0.19717926, 0.16727834, 0.19565669, 0.19581292, 0.21771053,\n",
       "       0.19309548, 0.19516679, 0.19499222, 0.20163254, 0.20112361,\n",
       "       0.2009406 , 0.18954442, 0.1798159 , 0.21139527, 0.18686511,\n",
       "       0.2016994 , 0.18963624, 0.20157894, 0.1984356 , 0.18229464])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FORMULA-2 DENOMINATOR\n",
    "one_sum=one_student.sum(axis=0)\n",
    "# print(one_sum)\n",
    "x=one_student/one_sum\n",
    "x[x!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 0.0\n",
      "## 0.12345679012345678\n",
      "## 0.24691358024691357\n",
      "## 0.3703703703703704\n",
      "## 0.49382716049382713\n",
      "## 0.6172839506172839\n",
      "## 0.7407407407407408\n",
      "## 0.8641975308641975\n",
      "## 0.9876543209876543\n",
      "## 1.1111111111111112\n",
      "## 1.2345679012345678\n",
      "## 1.3580246913580247\n",
      "## 1.4814814814814816\n",
      "## 1.6049382716049383\n",
      "## 1.728395061728395\n",
      "## 1.8518518518518516\n",
      "## 1.9753086419753085\n",
      "## 2.0987654320987654\n",
      "## 2.2222222222222223\n",
      "## 2.345679012345679\n",
      "## 2.4691358024691357\n",
      "## 2.5925925925925926\n",
      "## 2.7160493827160495\n",
      "## 2.8395061728395063\n",
      "## 2.9629629629629632\n",
      "## 3.0864197530864197\n",
      "## 3.2098765432098766\n",
      "## 3.3333333333333335\n",
      "## 3.45679012345679\n",
      "## 3.580246913580247\n",
      "## 3.7037037037037033\n",
      "## 3.8271604938271606\n",
      "## 3.950617283950617\n",
      "## 4.074074074074074\n",
      "## 4.197530864197531\n",
      "## 4.320987654320987\n",
      "## 4.444444444444445\n",
      "## 4.567901234567901\n",
      "## 4.691358024691358\n",
      "## 4.814814814814815\n",
      "## 4.938271604938271\n",
      "## 5.061728395061729\n",
      "## 5.185185185185185\n",
      "## 5.3086419753086425\n",
      "## 5.432098765432099\n",
      "## 5.555555555555555\n",
      "## 5.679012345679013\n",
      "## 5.802469135802469\n",
      "## 5.9259259259259265\n",
      "## 6.049382716049383\n",
      "## 6.172839506172839\n",
      "## 6.296296296296296\n",
      "## 6.419753086419753\n",
      "## 6.5432098765432105\n",
      "## 6.666666666666667\n",
      "## 6.790123456790123\n",
      "## 6.91358024691358\n",
      "## 7.037037037037037\n",
      "## 7.160493827160494\n",
      "## 7.28395061728395\n",
      "## 7.4074074074074066\n",
      "## 7.530864197530865\n",
      "## 7.654320987654321\n",
      "## 7.777777777777778\n",
      "## 7.901234567901234\n",
      "## 8.024691358024691\n",
      "## 8.148148148148149\n",
      "## 8.271604938271606\n",
      "## 8.395061728395062\n",
      "## 8.518518518518519\n",
      "## 8.641975308641975\n",
      "## 8.765432098765432\n",
      "## 8.88888888888889\n",
      "## 9.012345679012345\n",
      "## 9.135802469135802\n",
      "## 9.25925925925926\n",
      "## 9.382716049382717\n",
      "## 9.506172839506172\n",
      "## 9.62962962962963\n",
      "## 9.753086419753085\n",
      "## 9.876543209876543\n",
      "## 10.0\n",
      "## 10.123456790123457\n",
      "## 10.246913580246913\n",
      "## 10.37037037037037\n",
      "## 10.493827160493826\n",
      "## 10.617283950617285\n",
      "## 10.74074074074074\n",
      "## 10.864197530864198\n",
      "## 10.987654320987653\n",
      "## 11.11111111111111\n",
      "## 11.234567901234568\n",
      "## 11.358024691358025\n",
      "## 11.481481481481481\n",
      "## 11.604938271604938\n",
      "## 11.728395061728394\n",
      "## 11.851851851851853\n",
      "## 11.975308641975309\n",
      "## 12.098765432098766\n",
      "## 12.222222222222221\n",
      "## 12.345679012345679\n",
      "## 12.469135802469136\n",
      "## 12.592592592592592\n",
      "## 12.716049382716049\n",
      "## 12.839506172839506\n",
      "## 12.962962962962962\n",
      "## 13.086419753086421\n",
      "## 13.209876543209875\n",
      "## 13.333333333333334\n",
      "## 13.456790123456791\n",
      "## 13.580246913580247\n",
      "## 13.703703703703704\n",
      "## 13.82716049382716\n",
      "## 13.950617283950617\n",
      "## 14.074074074074074\n",
      "## 14.19753086419753\n",
      "## 14.320987654320987\n",
      "## 14.444444444444443\n",
      "## 14.5679012345679\n",
      "## 14.69135802469136\n",
      "## 14.814814814814813\n",
      "## 14.938271604938272\n",
      "## 15.06172839506173\n",
      "## 15.185185185185185\n",
      "## 15.308641975308642\n",
      "## 15.432098765432098\n",
      "## 15.555555555555555\n",
      "## 15.679012345679014\n",
      "## 15.802469135802468\n",
      "## 15.925925925925927\n",
      "## 16.049382716049383\n",
      "## 16.17283950617284\n",
      "## 16.296296296296298\n",
      "## 16.419753086419753\n",
      "## 16.543209876543212\n",
      "## 16.666666666666664\n",
      "## 16.790123456790123\n",
      "## 16.913580246913583\n",
      "## 17.037037037037038\n",
      "## 17.160493827160494\n",
      "## 17.28395061728395\n",
      "## 17.40740740740741\n",
      "## 17.530864197530864\n",
      "## 17.65432098765432\n",
      "## 17.77777777777778\n",
      "## 17.901234567901234\n",
      "## 18.02469135802469\n",
      "## 18.14814814814815\n",
      "## 18.271604938271604\n",
      "## 18.395061728395063\n",
      "## 18.51851851851852\n",
      "## 18.641975308641975\n",
      "## 18.765432098765434\n",
      "## 18.88888888888889\n",
      "## 19.012345679012345\n",
      "## 19.1358024691358\n",
      "## 19.25925925925926\n",
      "## 19.38271604938272\n",
      "## 19.50617283950617\n",
      "## 19.62962962962963\n",
      "## 19.753086419753085\n",
      "## 19.876543209876544\n",
      "## 20.0\n",
      "## 20.123456790123456\n",
      "## 20.246913580246915\n",
      "## 20.37037037037037\n",
      "## 20.493827160493826\n",
      "## 20.617283950617285\n",
      "## 20.74074074074074\n",
      "## 20.8641975308642\n",
      "## 20.98765432098765\n",
      "## 21.11111111111111\n",
      "## 21.23456790123457\n",
      "## 21.358024691358025\n",
      "## 21.48148148148148\n",
      "## 21.604938271604937\n",
      "## 21.728395061728396\n",
      "## 21.85185185185185\n",
      "## 21.975308641975307\n",
      "## 22.098765432098766\n",
      "## 22.22222222222222\n",
      "## 22.34567901234568\n",
      "## 22.469135802469136\n",
      "## 22.59259259259259\n",
      "## 22.71604938271605\n",
      "## 22.839506172839506\n",
      "## 22.962962962962962\n",
      "## 23.08641975308642\n",
      "## 23.209876543209877\n",
      "## 23.333333333333332\n",
      "## 23.456790123456788\n",
      "## 23.580246913580247\n",
      "## 23.703703703703706\n",
      "## 23.827160493827158\n",
      "## 23.950617283950617\n",
      "## 24.074074074074073\n",
      "## 24.19753086419753\n",
      "## 24.320987654320987\n",
      "## 24.444444444444443\n",
      "## 24.567901234567902\n",
      "## 24.691358024691358\n",
      "## 24.814814814814813\n",
      "## 24.938271604938272\n",
      "## 25.061728395061728\n",
      "## 25.185185185185183\n",
      "## 25.308641975308642\n",
      "## 25.432098765432098\n",
      "## 25.555555555555554\n",
      "## 25.679012345679013\n",
      "## 25.80246913580247\n",
      "## 25.925925925925924\n",
      "## 26.049382716049386\n",
      "## 26.172839506172842\n",
      "## 26.296296296296294\n",
      "## 26.41975308641975\n",
      "## 26.543209876543212\n",
      "## 26.666666666666668\n",
      "## 26.79012345679012\n",
      "## 26.913580246913583\n",
      "## 27.037037037037038\n",
      "## 27.160493827160494\n",
      "## 27.283950617283953\n",
      "## 27.40740740740741\n",
      "## 27.530864197530864\n",
      "## 27.65432098765432\n",
      "## 27.77777777777778\n",
      "## 27.901234567901234\n",
      "## 28.02469135802469\n",
      "## 28.14814814814815\n",
      "## 28.271604938271604\n",
      "## 28.39506172839506\n",
      "## 28.51851851851852\n",
      "## 28.641975308641975\n",
      "## 28.76543209876543\n",
      "## 28.888888888888886\n",
      "## 29.01234567901235\n",
      "## 29.1358024691358\n",
      "## 29.259259259259256\n",
      "## 29.38271604938272\n",
      "## 29.506172839506174\n",
      "## 29.629629629629626\n",
      "## 29.75308641975309\n",
      "## 29.876543209876544\n",
      "## 30.0\n",
      "## 30.12345679012346\n",
      "## 30.246913580246915\n",
      "## 30.37037037037037\n",
      "## 30.493827160493826\n",
      "## 30.617283950617285\n",
      "## 30.74074074074074\n",
      "## 30.864197530864196\n",
      "## 30.987654320987655\n",
      "## 31.11111111111111\n",
      "## 31.234567901234566\n",
      "## 31.35802469135803\n",
      "## 31.48148148148148\n",
      "## 31.604938271604937\n",
      "## 31.728395061728392\n",
      "## 31.851851851851855\n",
      "## 31.97530864197531\n",
      "## 32.098765432098766\n",
      "## 32.22222222222222\n",
      "## 32.34567901234568\n",
      "## 32.46913580246913\n",
      "## 32.592592592592595\n",
      "## 32.71604938271605\n",
      "## 32.839506172839506\n",
      "## 32.96296296296296\n",
      "## 33.086419753086425\n",
      "## 33.20987654320987\n",
      "## 33.33333333333333\n",
      "## 33.45679012345679\n",
      "## 33.58024691358025\n",
      "## 33.7037037037037\n",
      "## 33.827160493827165\n",
      "## 33.95061728395062\n",
      "## 34.074074074074076\n",
      "## 34.19753086419753\n",
      "## 34.32098765432099\n",
      "## 34.44444444444444\n",
      "## 34.5679012345679\n",
      "## 34.69135802469136\n",
      "## 34.81481481481482\n",
      "## 34.93827160493827\n",
      "## 35.06172839506173\n",
      "## 35.18518518518518\n",
      "## 35.30864197530864\n",
      "## 35.432098765432094\n",
      "## 35.55555555555556\n",
      "## 35.67901234567901\n",
      "## 35.80246913580247\n",
      "## 35.92592592592593\n",
      "## 36.04938271604938\n",
      "## 36.172839506172835\n",
      "## 36.2962962962963\n",
      "## 36.41975308641975\n",
      "## 36.54320987654321\n",
      "## 36.666666666666664\n",
      "## 36.79012345679013\n",
      "## 36.91358024691358\n",
      "## 37.03703703703704\n",
      "## 37.160493827160494\n",
      "## 37.28395061728395\n",
      "## 37.407407407407405\n",
      "## 37.53086419753087\n",
      "## 37.65432098765432\n",
      "## 37.77777777777778\n",
      "## 37.901234567901234\n",
      "## 38.02469135802469\n",
      "## 38.148148148148145\n",
      "## 38.2716049382716\n",
      "## 38.39506172839506\n",
      "## 38.51851851851852\n",
      "## 38.641975308641975\n",
      "## 38.76543209876544\n",
      "## 38.88888888888889\n",
      "## 39.01234567901234\n",
      "## 39.1358024691358\n",
      "## 39.25925925925926\n",
      "## 39.382716049382715\n",
      "## 39.50617283950617\n",
      "## 39.62962962962963\n",
      "## 39.75308641975309\n",
      "## 39.876543209876544\n",
      "## 40.0\n",
      "## 40.123456790123456\n",
      "## 40.24691358024691\n",
      "## 40.370370370370374\n",
      "## 40.49382716049383\n",
      "## 40.617283950617285\n",
      "## 40.74074074074074\n",
      "## 40.864197530864196\n",
      "## 40.98765432098765\n",
      "## 41.11111111111111\n",
      "## 41.23456790123457\n",
      "## 41.358024691358025\n",
      "## 41.48148148148148\n",
      "## 41.604938271604944\n",
      "## 41.7283950617284\n",
      "## 41.85185185185185\n",
      "## 41.9753086419753\n",
      "## 42.098765432098766\n",
      "## 42.22222222222222\n",
      "## 42.34567901234568\n",
      "## 42.46913580246914\n",
      "## 42.592592592592595\n",
      "## 42.71604938271605\n",
      "## 42.839506172839506\n",
      "## 42.96296296296296\n",
      "## 43.08641975308642\n",
      "## 43.20987654320987\n",
      "## 43.333333333333336\n",
      "## 43.45679012345679\n",
      "## 43.58024691358025\n",
      "## 43.7037037037037\n",
      "## 43.82716049382716\n",
      "## 43.95061728395061\n",
      "## 44.074074074074076\n",
      "## 44.19753086419753\n",
      "## 44.32098765432099\n",
      "## 44.44444444444444\n",
      "## 44.567901234567906\n",
      "## 44.69135802469136\n",
      "## 44.81481481481481\n",
      "## 44.93827160493827\n",
      "## 45.06172839506173\n",
      "## 45.18518518518518\n",
      "## 45.308641975308646\n",
      "## 45.4320987654321\n",
      "## 45.55555555555556\n",
      "## 45.67901234567901\n",
      "## 45.80246913580247\n",
      "## 45.925925925925924\n",
      "## 46.04938271604938\n",
      "## 46.17283950617284\n",
      "## 46.2962962962963\n",
      "## 46.41975308641975\n",
      "## 46.543209876543216\n",
      "## 46.666666666666664\n",
      "## 46.79012345679012\n",
      "## 46.913580246913575\n",
      "## 47.03703703703704\n",
      "## 47.160493827160494\n",
      "## 47.28395061728395\n",
      "## 47.40740740740741\n",
      "## 47.53086419753087\n",
      "## 47.654320987654316\n",
      "## 47.77777777777778\n",
      "## 47.901234567901234\n",
      "## 48.02469135802469\n",
      "## 48.148148148148145\n",
      "## 48.27160493827161\n",
      "## 48.39506172839506\n",
      "## 48.51851851851852\n",
      "## 48.641975308641975\n",
      "## 48.76543209876543\n",
      "## 48.888888888888886\n",
      "## 49.01234567901235\n",
      "## 49.135802469135804\n",
      "## 49.25925925925926\n",
      "## 49.382716049382715\n",
      "## 49.50617283950617\n",
      "## 49.629629629629626\n",
      "## 49.75308641975308\n",
      "## 49.876543209876544\n",
      "## 50.0\n",
      "## 50.123456790123456\n",
      "## 50.24691358024691\n",
      "## 50.37037037037037\n",
      "## 50.49382716049383\n",
      "## 50.617283950617285\n",
      "## 50.74074074074074\n",
      "## 50.864197530864196\n",
      "## 50.98765432098765\n",
      "## 51.11111111111111\n",
      "## 51.23456790123457\n",
      "## 51.358024691358025\n",
      "## 51.48148148148148\n",
      "## 51.60493827160494\n",
      "## 51.72839506172839\n",
      "## 51.85185185185185\n",
      "## 51.9753086419753\n",
      "## 52.09876543209877\n",
      "## 52.22222222222223\n",
      "## 52.345679012345684\n",
      "## 52.46913580246913\n",
      "## 52.59259259259259\n",
      "## 52.716049382716044\n",
      "## 52.8395061728395\n",
      "## 52.96296296296297\n",
      "## 53.086419753086425\n",
      "## 53.20987654320988\n",
      "## 53.333333333333336\n",
      "## 53.456790123456784\n",
      "## 53.58024691358024\n",
      "## 53.70370370370371\n",
      "## 53.827160493827165\n",
      "## 53.95061728395062\n",
      "## 54.074074074074076\n",
      "## 54.19753086419753\n",
      "## 54.32098765432099\n",
      "## 54.44444444444444\n",
      "## 54.567901234567906\n",
      "## 54.69135802469136\n",
      "## 54.81481481481482\n",
      "## 54.93827160493827\n",
      "## 55.06172839506173\n",
      "## 55.18518518518518\n",
      "## 55.30864197530864\n",
      "## 55.4320987654321\n",
      "## 55.55555555555556\n",
      "## 55.67901234567901\n",
      "## 55.80246913580247\n",
      "## 55.925925925925924\n",
      "## 56.04938271604938\n",
      "## 56.17283950617284\n",
      "## 56.2962962962963\n",
      "## 56.41975308641975\n",
      "## 56.54320987654321\n",
      "## 56.666666666666664\n",
      "## 56.79012345679012\n",
      "## 56.913580246913575\n",
      "## 57.03703703703704\n",
      "## 57.160493827160494\n",
      "## 57.28395061728395\n",
      "## 57.407407407407405\n",
      "## 57.53086419753086\n",
      "## 57.654320987654316\n",
      "## 57.77777777777777\n",
      "## 57.90123456790124\n",
      "## 58.0246913580247\n",
      "## 58.14814814814815\n",
      "## 58.2716049382716\n",
      "## 58.395061728395056\n",
      "## 58.51851851851851\n",
      "## 58.64197530864198\n",
      "## 58.76543209876544\n",
      "## 58.88888888888889\n",
      "## 59.01234567901235\n",
      "## 59.135802469135804\n",
      "## 59.25925925925925\n",
      "## 59.38271604938271\n",
      "## 59.50617283950618\n",
      "## 59.62962962962963\n",
      "## 59.75308641975309\n",
      "## 59.876543209876544\n",
      "## 60.0\n",
      "## 60.123456790123456\n",
      "## 60.24691358024692\n",
      "## 60.370370370370374\n",
      "## 60.49382716049383\n",
      "## 60.617283950617285\n",
      "## 60.74074074074074\n",
      "## 60.864197530864196\n",
      "## 60.98765432098765\n",
      "## 61.111111111111114\n",
      "## 61.23456790123457\n",
      "## 61.358024691358025\n",
      "## 61.48148148148148\n",
      "## 61.60493827160494\n",
      "## 61.72839506172839\n",
      "## 61.85185185185185\n",
      "## 61.97530864197531\n",
      "## 62.098765432098766\n",
      "## 62.22222222222222\n",
      "## 62.34567901234568\n",
      "## 62.46913580246913\n",
      "## 62.59259259259259\n",
      "## 62.71604938271606\n",
      "## 62.839506172839506\n",
      "## 62.96296296296296\n",
      "## 63.08641975308642\n",
      "## 63.20987654320987\n",
      "## 63.33333333333333\n",
      "## 63.456790123456784\n",
      "## 63.580246913580254\n",
      "## 63.70370370370371\n",
      "## 63.827160493827165\n",
      "## 63.95061728395062\n",
      "## 64.07407407407408\n",
      "## 64.19753086419753\n",
      "## 64.32098765432099\n",
      "## 64.44444444444444\n",
      "## 64.5679012345679\n",
      "## 64.69135802469135\n",
      "## 64.81481481481481\n",
      "## 64.93827160493827\n",
      "## 65.06172839506172\n",
      "## 65.18518518518519\n",
      "## 65.30864197530865\n",
      "## 65.4320987654321\n",
      "## 65.55555555555556\n",
      "## 65.67901234567901\n",
      "## 65.80246913580247\n",
      "## 65.92592592592592\n",
      "## 66.0493827160494\n",
      "## 66.17283950617285\n",
      "## 66.2962962962963\n",
      "## 66.41975308641975\n",
      "## 66.5432098765432\n",
      "## 66.66666666666666\n",
      "## 66.79012345679011\n",
      "## 66.91358024691358\n",
      "## 67.03703703703704\n",
      "## 67.1604938271605\n",
      "## 67.28395061728395\n",
      "## 67.4074074074074\n",
      "## 67.53086419753086\n",
      "## 67.65432098765433\n",
      "## 67.77777777777779\n",
      "## 67.90123456790124\n",
      "## 68.0246913580247\n",
      "## 68.14814814814815\n",
      "## 68.27160493827161\n",
      "## 68.39506172839506\n",
      "## 68.51851851851852\n",
      "## 68.64197530864197\n",
      "## 68.76543209876543\n",
      "## 68.88888888888889\n",
      "## 69.01234567901234\n",
      "## 69.1358024691358\n",
      "## 69.25925925925925\n",
      "## 69.38271604938272\n",
      "## 69.50617283950618\n",
      "## 69.62962962962963\n",
      "## 69.75308641975309\n",
      "## 69.87654320987654\n",
      "## 70.0\n",
      "## 70.12345679012346\n",
      "## 70.24691358024691\n",
      "## 70.37037037037037\n",
      "## 70.49382716049382\n",
      "## 70.61728395061728\n",
      "## 70.74074074074073\n",
      "## 70.86419753086419\n",
      "## 70.98765432098766\n",
      "## 71.11111111111111\n",
      "## 71.23456790123457\n",
      "## 71.35802469135803\n",
      "## 71.48148148148148\n",
      "## 71.60493827160494\n",
      "## 71.7283950617284\n",
      "## 71.85185185185186\n",
      "## 71.97530864197532\n",
      "## 72.09876543209876\n",
      "## 72.22222222222221\n",
      "## 72.34567901234567\n",
      "## 72.46913580246913\n",
      "## 72.5925925925926\n",
      "## 72.71604938271605\n",
      "## 72.8395061728395\n",
      "## 72.96296296296296\n",
      "## 73.08641975308642\n",
      "## 73.20987654320987\n",
      "## 73.33333333333333\n",
      "## 73.4567901234568\n",
      "## 73.58024691358025\n",
      "## 73.70370370370371\n",
      "## 73.82716049382717\n",
      "## 73.95061728395062\n",
      "## 74.07407407407408\n",
      "## 74.19753086419753\n",
      "## 74.32098765432099\n",
      "## 74.44444444444444\n",
      "## 74.5679012345679\n",
      "## 74.69135802469135\n",
      "## 74.81481481481481\n",
      "## 74.93827160493827\n",
      "## 75.06172839506173\n",
      "## 75.18518518518519\n",
      "## 75.30864197530865\n",
      "## 75.4320987654321\n",
      "## 75.55555555555556\n",
      "## 75.67901234567901\n",
      "## 75.80246913580247\n",
      "## 75.92592592592592\n",
      "## 76.04938271604938\n",
      "## 76.17283950617283\n",
      "## 76.29629629629629\n",
      "## 76.41975308641975\n",
      "## 76.5432098765432\n",
      "## 76.66666666666667\n",
      "## 76.79012345679013\n",
      "## 76.91358024691358\n",
      "## 77.03703703703704\n",
      "## 77.1604938271605\n",
      "## 77.28395061728395\n",
      "## 77.4074074074074\n",
      "## 77.53086419753087\n",
      "## 77.65432098765433\n",
      "## 77.77777777777779\n",
      "## 77.90123456790124\n",
      "## 78.02469135802468\n",
      "## 78.14814814814814\n",
      "## 78.2716049382716\n",
      "## 78.39506172839506\n",
      "## 78.51851851851852\n",
      "## 78.64197530864197\n",
      "## 78.76543209876543\n",
      "## 78.88888888888889\n",
      "## 79.01234567901234\n",
      "## 79.13580246913581\n",
      "## 79.25925925925927\n",
      "## 79.38271604938272\n",
      "## 79.50617283950618\n",
      "## 79.62962962962963\n",
      "## 79.75308641975309\n",
      "## 79.87654320987654\n",
      "## 80.0\n",
      "## 80.12345679012346\n",
      "## 80.24691358024691\n",
      "## 80.37037037037037\n",
      "## 80.49382716049382\n",
      "## 80.61728395061728\n",
      "## 80.74074074074075\n",
      "## 80.8641975308642\n",
      "## 80.98765432098766\n",
      "## 81.11111111111111\n",
      "## 81.23456790123457\n",
      "## 81.35802469135803\n",
      "## 81.48148148148148\n",
      "## 81.60493827160494\n",
      "## 81.72839506172839\n",
      "## 81.85185185185185\n",
      "## 81.9753086419753\n",
      "## 82.09876543209876\n",
      "## 82.22222222222221\n",
      "## 82.34567901234567\n",
      "## 82.46913580246914\n",
      "## 82.5925925925926\n",
      "## 82.71604938271605\n",
      "## 82.8395061728395\n",
      "## 82.96296296296296\n",
      "## 83.08641975308642\n",
      "## 83.20987654320989\n",
      "## 83.33333333333334\n",
      "## 83.4567901234568\n",
      "## 83.58024691358025\n",
      "## 83.7037037037037\n",
      "## 83.82716049382715\n",
      "## 83.9506172839506\n",
      "## 84.07407407407408\n",
      "## 84.19753086419753\n",
      "## 84.32098765432099\n",
      "## 84.44444444444444\n",
      "## 84.5679012345679\n",
      "## 84.69135802469135\n",
      "## 84.81481481481481\n",
      "## 84.93827160493828\n",
      "## 85.06172839506173\n",
      "## 85.18518518518519\n",
      "## 85.30864197530865\n",
      "## 85.4320987654321\n",
      "## 85.55555555555556\n",
      "## 85.67901234567901\n",
      "## 85.80246913580247\n",
      "## 85.92592592592592\n",
      "## 86.04938271604938\n",
      "## 86.17283950617283\n",
      "## 86.29629629629629\n",
      "## 86.41975308641975\n",
      "## 86.54320987654322\n",
      "## 86.66666666666667\n",
      "## 86.79012345679013\n",
      "## 86.91358024691358\n",
      "## 87.03703703703704\n",
      "## 87.1604938271605\n",
      "## 87.28395061728395\n",
      "## 87.4074074074074\n",
      "## 87.53086419753086\n",
      "## 87.65432098765432\n",
      "## 87.77777777777777\n",
      "## 87.90123456790123\n",
      "## 88.02469135802468\n",
      "## 88.14814814814815\n",
      "## 88.27160493827161\n",
      "## 88.39506172839506\n",
      "## 88.51851851851852\n",
      "## 88.64197530864197\n",
      "## 88.76543209876543\n",
      "## 88.88888888888889\n",
      "## 89.01234567901236\n",
      "## 89.13580246913581\n",
      "## 89.25925925925927\n",
      "## 89.38271604938272\n",
      "## 89.50617283950618\n",
      "## 89.62962962962962\n",
      "## 89.75308641975307\n",
      "## 89.87654320987654\n",
      "## 90.0\n",
      "## 90.12345679012346\n",
      "## 90.24691358024691\n",
      "## 90.37037037037037\n",
      "## 90.49382716049382\n",
      "## 90.61728395061729\n",
      "## 90.74074074074075\n",
      "## 90.8641975308642\n",
      "## 90.98765432098766\n",
      "## 91.11111111111111\n",
      "## 91.23456790123457\n",
      "## 91.35802469135803\n",
      "## 91.48148148148148\n",
      "## 91.60493827160494\n",
      "## 91.72839506172839\n",
      "## 91.85185185185185\n",
      "## 91.9753086419753\n",
      "## 92.09876543209876\n",
      "## 92.22222222222223\n",
      "## 92.34567901234568\n",
      "## 92.46913580246914\n",
      "## 92.5925925925926\n",
      "## 92.71604938271605\n",
      "## 92.8395061728395\n",
      "## 92.96296296296296\n",
      "## 93.08641975308643\n",
      "## 93.20987654320987\n",
      "## 93.33333333333333\n",
      "## 93.45679012345678\n",
      "## 93.58024691358024\n",
      "## 93.7037037037037\n",
      "## 93.82716049382715\n",
      "## 93.95061728395062\n",
      "## 94.07407407407408\n",
      "## 94.19753086419753\n",
      "## 94.32098765432099\n",
      "## 94.44444444444444\n",
      "## 94.5679012345679\n",
      "## 94.69135802469137\n",
      "## 94.81481481481482\n",
      "## 94.93827160493828\n",
      "## 95.06172839506173\n",
      "## 95.18518518518519\n",
      "## 95.30864197530863\n",
      "## 95.43209876543209\n",
      "## 95.55555555555556\n",
      "## 95.67901234567901\n",
      "## 95.80246913580247\n",
      "## 95.92592592592592\n",
      "## 96.04938271604938\n",
      "## 96.17283950617283\n",
      "## 96.29629629629629\n",
      "## 96.41975308641976\n",
      "## 96.54320987654322\n",
      "## 96.66666666666667\n",
      "## 96.79012345679013\n",
      "## 96.91358024691358\n",
      "## 97.03703703703704\n",
      "## 97.1604938271605\n",
      "## 97.28395061728395\n",
      "## 97.4074074074074\n",
      "## 97.53086419753086\n",
      "## 97.65432098765432\n",
      "## 97.77777777777777\n",
      "## 97.90123456790123\n",
      "## 98.0246913580247\n",
      "## 98.14814814814815\n",
      "## 98.27160493827161\n",
      "## 98.39506172839506\n",
      "## 98.51851851851852\n",
      "## 98.64197530864197\n",
      "## 98.76543209876543\n",
      "## 98.88888888888889\n",
      "## 99.01234567901234\n",
      "## 99.1358024691358\n",
      "## 99.25925925925925\n",
      "## 99.38271604938271\n",
      "## 99.50617283950616\n",
      "## 99.62962962962963\n",
      "## 99.75308641975309\n",
      "## 99.87654320987654\n"
     ]
    }
   ],
   "source": [
    "#CALCULATE FORMULA-2 FOR ALL CANDIDATES IN DATA\n",
    "relation_matrix_improved=np.zeros((nb_skills,nb_skills))\n",
    "relation_matrix_improved_v=np.zeros((nb_skills,nb_skills))\n",
    "for index,candidate_pred in enumerate(data_set_prediction):\n",
    "    relation_matrix_improved=calculate_one_relation_improved(nb_skills,candidate_pred,data_set_student_answer[index],\n",
    "                                                             relation_matrix_improved)\n",
    "    divide_vector = relation_matrix_improved.sum(axis=0)\n",
    "    if 0 in divide_vector:\n",
    "        divide_vector[divide_vector==0]=1\n",
    "    relation_matrix_improved_v += relation_matrix_improved / divide_vector\n",
    "    print(\"##\",index/len(data_set_prediction)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.77680027,  4.25135889,  3.69064131, ...,  4.14028267,\n",
       "         3.71265451,  3.40814717],\n",
       "       [12.42878287, 14.62805508, 12.96438048, ..., 13.98543655,\n",
       "        13.25285189, 12.42387476],\n",
       "       [ 1.45430941,  2.39124372,  2.04468969, ...,  1.79999023,\n",
       "         1.4643537 ,  1.31434496],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.14322313,  2.04499821,  2.4111015 , ...,  2.36541069,\n",
       "         2.52931812,  2.18537731],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRINT RELATION MATRIX FOR FORMULA-2 WITHOUT NORMALIZATION\n",
    "relation_matrix_improved_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.02736812,  0.03080695,  0.02674378, ...,  0.03000205,\n         0.02690329,  0.02469672],\n       [ 0.05675243,  0.06679477,  0.05919808, ...,  0.06386044,\n         0.06051531,  0.05673002],\n       [ 0.01411951,  0.02321596,  0.01985136, ...,  0.01747563,\n         0.01421703,  0.01276063],\n       ...,\n       [-0.        , -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       [ 0.02408116,  0.02297751,  0.02709103, ...,  0.02657765,\n         0.0284193 ,  0.0245548 ],\n       [-0.        , -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ]])"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_one_relation_improved_normalize=relation_matrix_improved_v/counter_matrix\n",
    "calculate_one_relation_improved_normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}